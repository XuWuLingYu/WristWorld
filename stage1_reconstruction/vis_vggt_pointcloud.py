#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VGGTç‚¹äº‘å¯è§†åŒ–è„šæœ¬

è¾“å…¥ï¼š
- VGGT checkpointè·¯å¾„
- ext1è§†é¢‘è·¯å¾„
- ext2è§†é¢‘è·¯å¾„

åŠŸèƒ½ï¼š
1. åŠ è½½VGGTæ¨¡å‹ï¼ˆä½¿ç”¨ä¸prepare_condition_clips.pyç›¸åŒçš„å‚æ•°ï¼‰
2. æ¨ç†é¦–å¸§ç‚¹äº‘
3. ä½¿ç”¨ext1ã€ext2å¤–å‚çš„çƒé¢æ’å€¼å‡å€¼
4. ä½¿ç”¨ext1å†…å‚
5. æ¸²æŸ“1920x1080é«˜æ¸…å›¾åƒï¼ŒèƒŒæ™¯é€æ˜
6. ä¿å­˜PNG

çƒé¢æ’å€¼è¯´æ˜ï¼š
- å°†æ—‹è½¬çŸ©é˜µè½¬æ¢ä¸ºå››å…ƒæ•°
- å¯¹å››å…ƒæ•°è¿›è¡Œçƒé¢çº¿æ€§æ’å€¼ï¼ˆSLERPï¼‰
- å¯¹å¹³ç§»å‘é‡è¿›è¡Œçº¿æ€§æ’å€¼
- é‡æ–°ç»„åˆä¸ºå¤–å‚çŸ©é˜µ
"""

import os
import sys
import argparse
import numpy as np
import cv2
from PIL import Image
import torch
import torch.nn.functional as F
from pathlib import Path
from typing import Tuple, List, Optional
import json
from scipy.spatial.transform import Rotation as R
from scipy.spatial.transform import Slerp
import trimesh

# æ·»åŠ VGGTè·¯å¾„
sys.path.append('/mnt/zezhong/vggt_training')
sys.path.append('/mnt/zezhong/vggt_training/vggt')
sys.path.append('/mnt/zezhong/vggt_training/training')

try:
    from vggt.models.vggt import VGGT
    from vggt.utils.load_fn import load_and_preprocess_images
    from vggt.utils.pose_enc import pose_encoding_to_extri_intri, extri_intri_to_pose_encoding
    from vggt.utils.geometry import unproject_depth_map_to_point_map
except ImportError as e:
    print(f"å¯¼å…¥VGGTæ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿è„šæœ¬åœ¨æ­£ç¡®çš„ç¯å¢ƒä¸­è¿è¡Œï¼Œå¹¶ä¸”è·¯å¾„è®¾ç½®æ­£ç¡®")
    sys.exit(1)


class VGGTPointCloudVisualizer:
    def __init__(self, checkpoint_path: str, device: str = "cuda"):
        """
        åˆå§‹åŒ–VGGTç‚¹äº‘å¯è§†åŒ–å™¨
        
        Args:
            checkpoint_path: æ¨¡å‹checkpointè·¯å¾„
            device: æ¨ç†è®¾å¤‡
        """
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        self.checkpoint_path = checkpoint_path
        self.model = self._load_model()
        
        print(f"âœ… VGGTç‚¹äº‘å¯è§†åŒ–å™¨åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device}")
    
    def _load_model(self) -> VGGT:
        """
        åŠ è½½VGGTæ¨¡å‹ï¼ˆä¸prepare_condition_clips.pyå®Œå…¨ä¸€è‡´ï¼‰
        
        Returns:
            åŠ è½½çš„VGGTæ¨¡å‹
        """
        print(f"ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹checkpoint: {self.checkpoint_path}")
        
        # ä¸prepare_condition_clips.pyå®Œå…¨ä¸€è‡´çš„æ¨¡å‹é…ç½®
        model = VGGT(
            img_size=518,
            patch_size=14,
            embed_dim=1024,
            enable_camera=True,      # éœ€è¦ç›¸æœºå‚æ•°é¢„æµ‹
            enable_depth=True,       # éœ€è¦æ·±åº¦é¢„æµ‹
            enable_point=True,       # éœ€è¦ç‚¹äº‘é¢„æµ‹
            enable_track=False,      # ä¸éœ€è¦tracké¢„æµ‹
            enable_wrist=True,       # éœ€è¦wrist poseé¢„æµ‹
            pretrained="facebook/VGGT-1B",
            use_lora=False,          # ä¸ä½¿ç”¨LoRA
            lora_rank=16,
            lora_alpha=32
        )
        
        # ç¦ç”¨track_headå‚æ•°çš„æ¢¯åº¦ï¼ˆä¸è®­ç»ƒé…ç½®ä¸€è‡´ï¼‰
        for name, param in model.named_parameters():
            if "track_head" in name:
                param.requires_grad = False
        
        # åŠ è½½checkpoint
        try:
            checkpoint = torch.load(self.checkpoint_path, map_location="cpu")
            
            if "model" in checkpoint:
                model_state_dict = checkpoint["model"]
            else:
                model_state_dict = checkpoint
                
            missing_keys, unexpected_keys = model.load_state_dict(model_state_dict, strict=False)
            
            if missing_keys:
                print(f"âš ï¸ ç¼ºå¤±çš„é”®: {missing_keys}")
            if unexpected_keys:
                print(f"âš ï¸ æœªé¢„æœŸçš„é”®: {unexpected_keys}")
                
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ åŠ è½½checkpointå¤±è´¥: {e}")
            print("ğŸ”„ å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
            try:
                model = VGGT.from_pretrained("facebook/VGGT-1B")
                print("âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e2:
                print(f"âŒ é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ä¹Ÿå¤±è´¥: {e2}")
                sys.exit(1)
        
        model.eval()
        model.to(self.device)
        return model
    
    def _get_video_frame_count(self, video_path: str) -> int:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        cap.release()
        return max(total, 0)

    def extract_first_frame(self, video_path: str,frame_num=0) -> np.ndarray:
        """
        æå–è§†é¢‘é¦–å¸§
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            é¦–å¸§RGBå›¾åƒ
        """
        print(f"ğŸ¬ æå–é¦–å¸§: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        for i in range(frame_num):
            ret, frame = cap.read()
            if not ret:
                raise ValueError(f"æ— æ³•è¯»å–è§†é¢‘é¦–å¸§: {video_path}")
        
        ret, frame = cap.read()
        if not ret:
            raise ValueError(f"æ— æ³•è¯»å–è§†é¢‘é¦–å¸§: {video_path}")
        
        # è½¬æ¢BGRåˆ°RGB
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        cap.release()
        
        print(f"âœ… é¦–å¸§æå–å®Œæˆï¼Œå°ºå¯¸: {frame_rgb.shape}")
        return frame_rgb
    
    def preprocess_frames(self, frame1: np.ndarray, frame2: np.ndarray) -> torch.Tensor:
        """
        é¢„å¤„ç†å¸§å¯¹ï¼ˆä¸prepare_condition_clips.pyä¸€è‡´ï¼‰
        
        Args:
            frame1: ç¬¬ä¸€å¸§
            frame2: ç¬¬äºŒå¸§
        """
        import tempfile, shutil
        temp_dir = Path(tempfile.mkdtemp(prefix=f"vggt_temp_{os.getpid()}_", dir=str(Path.cwd())))
        try:
            frame1_path = temp_dir / "frame1.jpg"
            frame2_path = temp_dir / "frame2.jpg"
            Image.fromarray(frame1).save(frame1_path)
            Image.fromarray(frame2).save(frame2_path)
            
            try:
                images = load_and_preprocess_images([str(frame1_path), str(frame2_path)])
            except ImportError:
                images = self._simple_preprocess_images([str(frame1_path), str(frame2_path)])
            
            return images.to(self.device)
        finally:
            try:
                shutil.rmtree(temp_dir, ignore_errors=True)
            except Exception:
                pass
    
    def _simple_preprocess_images(self, image_paths: List[str]) -> torch.Tensor:
        """
        ç®€åŒ–çš„å›¾åƒé¢„å¤„ç†ï¼ˆä¸prepare_condition_clips.pyä¸€è‡´ï¼‰
        """
        import torchvision.transforms as TF
        
        images = []
        to_tensor = TF.ToTensor()
        
        for image_path in image_paths:
            img = Image.open(image_path)
            if img.mode == "RGBA":
                background = Image.new("RGBA", img.size, (255, 255, 255, 255))
                img = Image.alpha_composite(background, img)
            img = img.convert("RGB")
            
            img = img.resize((518, 518), Image.Resampling.BICUBIC)
            img_tensor = to_tensor(img)
            images.append(img_tensor)
        
        return torch.stack(images)
    
    def run_inference(self, frame1: np.ndarray, frame2: np.ndarray) -> dict:
        """
        è¿è¡ŒVGGTæ¨ç†
        
        Args:
            frame1: ç¬¬ä¸€å¸§
            frame2: ç¬¬äºŒå¸§
            
        Returns:
            æ¨ç†ç»“æœ
        """
        print("ğŸ”„ è¿è¡ŒVGGTæ¨ç†...")
        
        # é¢„å¤„ç†
        images = self.preprocess_frames(frame1, frame2)
        images = images.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
        
        # æ¨ç†
        dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16
        
        with torch.no_grad():
            with torch.cuda.amp.autocast(dtype=dtype):
                predictions = self.model(images)
        
        # è½¬æ¢poseç¼–ç ä¸ºextrinsic/intrinsicçŸ©é˜µ
        if "pose_enc" in predictions:
            extrinsic, intrinsic = pose_encoding_to_extri_intri(
                predictions["pose_enc"], 
                image_size_hw=(294, 518),  # ä¸è®­ç»ƒæ—¶çš„å›¾åƒå°ºå¯¸ä¸€è‡´
                build_intrinsics=True
            )
            predictions["extrinsic"] = extrinsic
            predictions["intrinsic"] = intrinsic
        
        # è½¬æ¢wrist poseç¼–ç ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if "wrist_pose_enc" in predictions:
            wrist_extrinsic, wrist_intrinsic = pose_encoding_to_extri_intri(
                predictions["wrist_pose_enc"], 
                image_size_hw=(294, 518),  # ä¸è®­ç»ƒæ—¶çš„å›¾åƒå°ºå¯¸ä¸€è‡´
                build_intrinsics=True
            )
            predictions["wrist_extrinsic"] = wrist_extrinsic
            predictions["wrist_intrinsic"] = wrist_intrinsic
        
        # æ·»åŠ åŸå§‹images
        predictions["images"] = images.cpu().numpy()
        
        # è½¬æ¢ä¸ºnumpy
        for key in predictions.keys():
            if isinstance(predictions[key], torch.Tensor):
                predictions[key] = predictions[key].cpu().numpy()
        
        print("âœ… VGGTæ¨ç†å®Œæˆ")
        return predictions
    
    def spherical_interpolate_extrinsics(self, ext1: np.ndarray, ext2: np.ndarray, t: float = 0.5) -> np.ndarray:
        """
        ä½¿ç”¨çƒé¢æ’å€¼è®¡ç®—ä¸¤ä¸ªå¤–å‚çŸ©é˜µçš„æ’å€¼ï¼ˆæ”¯æŒå¤–æ¨ï¼‰
        
        Args:
            ext1: ç¬¬ä¸€ä¸ªå¤–å‚çŸ©é˜µ (3, 4)
            ext2: ç¬¬äºŒä¸ªå¤–å‚çŸ©é˜µ (3, 4)
            t: æ’å€¼å‚æ•°ï¼Œ0.5è¡¨ç¤ºä¸­ç‚¹ï¼Œ>1è¡¨ç¤ºå¤–æ¨ï¼Œ<0è¡¨ç¤ºåå‘å¤–æ¨
            
        Returns:
            æ’å€¼åçš„å¤–å‚çŸ©é˜µ (3, 4)
        """
        # æå–æ—‹è½¬çŸ©é˜µå’Œå¹³ç§»å‘é‡
        R1 = ext1[:3, :3]
        t1 = ext1[:3, 3]
        R2 = ext2[:3, :3]
        t2 = ext2[:3, 3]
        
        # å°†æ—‹è½¬çŸ©é˜µè½¬æ¢ä¸ºå››å…ƒæ•°
        r1 = R.from_matrix(R1)
        r2 = R.from_matrix(R2)
        
        # å¤„ç†å¤–æ¨æ’å€¼
        if t < 0 or t > 1:
            # å¯¹äºå¤–æ¨ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—ä»r1åˆ°r2çš„æ—‹è½¬å¢é‡
            # ç„¶åæŒ‰æ¯”ä¾‹åº”ç”¨è¿™ä¸ªå¢é‡
            r1_to_r2 = r2 * r1.inv()  # ä»r1åˆ°r2çš„ç›¸å¯¹æ—‹è½¬
            
            # è®¡ç®—æ—‹è½¬å¢é‡
            r_increment = r1_to_r2
            r_interp = r1 * (r_increment ** t)  # åº”ç”¨tå€çš„æ—‹è½¬å¢é‡
        else:
            # æ­£å¸¸æ’å€¼
            key_rots = R.concatenate([r1, r2])
            key_times = [0, 1]
            slerp = Slerp(key_times, key_rots)
            r_interp = slerp(t)
        
        # çº¿æ€§æ’å€¼å¹³ç§»å‘é‡ï¼ˆæ”¯æŒå¤–æ¨ï¼‰
        t_interp = (1 - t) * t1 + t * t2
        
        # é‡æ–°ç»„åˆå¤–å‚çŸ©é˜µ
        ext_interp = np.eye(4)
        ext_interp[:3, :3] = r_interp.as_matrix()
        ext_interp[:3, 3] = t_interp
        
        print(f"ğŸ”„ çƒé¢æ’å€¼ä¿¡æ¯:")
        print(f"  æ’å€¼å‚æ•° t: {t}")
        print(f"  æ’å€¼ç±»å‹: {'å¤–æ¨' if t < 0 or t > 1 else 'å†…æ’'}")
        print(f"  æ—‹è½¬è§’åº¦å·®: {np.linalg.norm(R1 - R2):.3f}")
        print(f"  å¹³ç§»è·ç¦»å·®: {np.linalg.norm(t1 - t2):.3f}")
        
        return ext_interp[:3, :4]  # è¿”å›3x4æ ¼å¼
    
    def _project_world_points_to_view(self,
                                      world_points: np.ndarray,
                                      view_extrinsic_w2c: np.ndarray,
                                      view_intrinsic: np.ndarray,
                                      img_size: Tuple[int, int]) -> np.ndarray:
        """
        å°†ä¸–ç•Œåæ ‡ç‚¹æŠ•å½±åˆ°å½“å‰è§†è§’å›¾åƒ
        Args:
            world_points: (N, 3)
            view_extrinsic_w2c: (3, 4) world->camera
            view_intrinsic: (3, 3)
            img_size: (H, W)
        Returns:
            åƒç´ åæ ‡ (N, 2)ï¼Œä¸å¯è§ç‚¹è¿”å›NaN
        """
        H, W = img_size
        ones = np.ones((world_points.shape[0], 1), dtype=world_points.dtype)
        pts_h = np.concatenate([world_points, ones], axis=1).T  # (4, N)

        T_view = np.vstack([view_extrinsic_w2c, [0, 0, 0, 1]])  # (4,4)
        cam_pts = (T_view @ pts_h)[:3].T  # (N,3)

        # ä»…æŠ•å½±åˆ°å‰æ–¹
        z = cam_pts[:, 2]
        valid = z > 0
        pixels = np.full((world_points.shape[0], 2), np.nan, dtype=np.float32)
        if not np.any(valid):
            return pixels

        uvw = (view_intrinsic @ cam_pts[valid].T).T  # (M,3)
        uv = uvw[:, :2] / uvw[:, 2:3]
        # è¾¹ç•Œæ£€æŸ¥
        in_bounds = (uv[:, 0] >= 0) & (uv[:, 0] < W) & (uv[:, 1] >= 0) & (uv[:, 1] < H)
        valid_idx = np.where(valid)[0]
        pixels[valid_idx[in_bounds]] = uv[in_bounds]
        return pixels

    def _draw_wrist_frustum(self,
                            image_rgba: np.ndarray,
                            wrist_extrinsic_w2c: np.ndarray,
                            wrist_intrinsic: np.ndarray,
                            view_extrinsic_w2c: np.ndarray,
                            view_intrinsic: np.ndarray,
                            img_size: Tuple[int, int] = (1080, 1920),
                            frustum_length: float = 0.2,
                            color_bgra: Tuple[int, int, int, int] = (0, 255, 0, 255),
                            thickness: int = 2) -> np.ndarray:
        """
        åœ¨æŠ•å½±å›¾åƒä¸Šç»˜åˆ¶åŸºäºwristç›¸æœºä½å§¿çš„ç›¸æœºå…‰é”¥ç¤ºæ„å›¾ã€‚
        - ä½¿ç”¨wristå†…å‚åœ¨å…¶æˆåƒå¹³é¢å››è§’å‘å°„å°„çº¿ï¼Œé•¿åº¦ä¸ºfrustum_lengthï¼ˆç›¸æœºåæ ‡ç³»å•ä½ï¼‰ã€‚
        - å°†è¿™äº›ç‚¹ä»wristç›¸æœºåæ ‡å˜æ¢åˆ°ä¸–ç•Œåæ ‡ï¼Œå†ç”¨å½“å‰è§†è§’æŠ•å½±åˆ°å›¾åƒä¸Šç»˜çº¿ã€‚
        """
        H_in, W_in = 294, 518  # ä¸ä¸Šæ¸¸ç”Ÿæˆwrist_intrinsicæ—¶çš„å¤§å°ä¸€è‡´
        # è®¡ç®—wristç›¸æœºçš„ç›¸æœºåˆ°ä¸–ç•Œå˜æ¢
        T_wrist = np.vstack([wrist_extrinsic_w2c, [0, 0, 0, 1]])  # world->wrist_cam
        T_wrist_inv = np.linalg.inv(T_wrist)  # wrist_cam->world

        # æ„é€ å››ä¸ªè§’ç‚¹çš„åƒç´ åæ ‡ä»¥åŠç›¸æœºåŸç‚¹
        corners_px = np.array([
            [0, 0, 1],
            [W_in - 1, 0, 1],
            [W_in - 1, H_in - 1, 1],
            [0, H_in - 1, 1],
        ], dtype=np.float32).T  # (3,4)

        K_inv = np.linalg.inv(wrist_intrinsic)
        rays_cam = K_inv @ corners_px  # (3,4)
        # å½’ä¸€åŒ–æ–¹å‘
        rays_cam = rays_cam / np.linalg.norm(rays_cam, axis=0, keepdims=True)

        # ç›¸æœºåŸç‚¹ä¸è§’ç‚¹(ç›¸æœºåæ ‡ç³»)
        origin_cam = np.array([[0, 0, 0, 1]], dtype=np.float32).T  # (4,1)
        corners_cam = np.vstack([rays_cam * frustum_length, np.ones((1, 4), dtype=np.float32)])  # (4,4)

        # å˜æ¢åˆ°ä¸–ç•Œåæ ‡
        origin_world = (T_wrist_inv @ origin_cam)[:3].T  # (1,3)
        corners_world = (T_wrist_inv @ corners_cam)[:3].T  # (4,3)

        # æŠ•å½±åˆ°å½“å‰è§†è§’å›¾åƒ
        pts_world = np.vstack([origin_world, corners_world])  # (5,3)
        uv = self._project_world_points_to_view(pts_world, view_extrinsic_w2c, view_intrinsic, img_size)

        # è½¬æ¢åˆ°BGRAä»¥ä¾¿cv2ç»˜åˆ¶
        img_bgra = cv2.cvtColor(image_rgba, cv2.COLOR_RGBA2BGRA)

        # è¿æ¥åŸç‚¹åˆ°å››ä¸ªè§’
        for j in range(1, 5):
            p0, p1 = uv[0], uv[j]
            if not (np.any(np.isnan(p0)) or np.any(np.isnan(p1))):
                cv2.line(img_bgra,
                         (int(round(p0[0])), int(round(p0[1]))),
                         (int(round(p1[0])), int(round(p1[1]))),
                         color_bgra, thickness)

        # è¿æ¥è§’ç‚¹ä¹‹é—´ä»¥å½¢æˆé”¥ä½“è¾¹æ¡†
        edges = [(1, 2), (2, 3), (3, 4), (4, 1)]
        for a, b in edges:
            p0, p1 = uv[a], uv[b]
            if not (np.any(np.isnan(p0)) or np.any(np.isnan(p1))):
                cv2.line(img_bgra,
                         (int(round(p0[0])), int(round(p0[1]))),
                         (int(round(p1[0])), int(round(p1[1]))),
                         color_bgra, thickness)

        return cv2.cvtColor(img_bgra, cv2.COLOR_BGRA2RGBA)

    def _adjust_intrinsic_for_larger_fov(self, intrinsic: np.ndarray, target_size: Tuple[int, int]) -> np.ndarray:
        """
        è°ƒæ•´å†…å‚ä»¥å¢å¤§è§†é‡ï¼ˆç„¦è·å‡åŠï¼Œä¸»ç‚¹è°ƒæ•´åˆ°ç›®æ ‡å›¾åƒä¸­å¿ƒï¼‰
        
        Args:
            intrinsic: åŸå§‹å†…å‚çŸ©é˜µ (3, 3)
            target_size: ç›®æ ‡å›¾åƒå°ºå¯¸ (H, W)
            
        Returns:
            è°ƒæ•´åçš„å†…å‚çŸ©é˜µ (3, 3)
        """
        H, W = target_size
        
        # åˆ›å»ºæ–°çš„å†…å‚çŸ©é˜µ
        new_intrinsic = intrinsic.copy()
        
        # ç„¦è·å‡åŠï¼ˆå¢å¤§ä¸€å€è§†é‡ï¼‰
        new_intrinsic[0, 0] = intrinsic[0, 0] * 3  # fx
        new_intrinsic[1, 1] = intrinsic[1, 1] * 3  # fy
        
        # ä¸»ç‚¹è°ƒæ•´åˆ°ç›®æ ‡å›¾åƒä¸­å¿ƒ
        new_intrinsic[0, 2] = W / 2.0-400  # cx
        new_intrinsic[1, 2] = H / 2.0-200  # cy
        
        print(f"ğŸ”§ å†…å‚è°ƒæ•´:")
        print(f"  åŸå§‹ç„¦è·: fx={intrinsic[0, 0]:.1f}, fy={intrinsic[1, 1]:.1f}")
        print(f"  åŸå§‹ä¸»ç‚¹: cx={intrinsic[0, 2]:.1f}, cy={intrinsic[1, 2]:.1f}")
        print(f"  è°ƒæ•´åç„¦è·: fx={new_intrinsic[0, 0]:.1f}, fy={new_intrinsic[1, 1]:.1f}")
        print(f"  è°ƒæ•´åä¸»ç‚¹: cx={new_intrinsic[0, 2]:.1f}, cy={new_intrinsic[1, 2]:.1f}")
        
        return new_intrinsic
    
    def _render_point(self, image: np.ndarray, u: int, v: int, color: np.ndarray, radius: int = 2):
        """
        åœ¨å›¾åƒä¸Šæ¸²æŸ“ä¸€ä¸ªç‚¹ï¼ˆå°åœ†å½¢ï¼‰
        
        Args:
            image: ç›®æ ‡å›¾åƒ (H, W, 4)
            u, v: åƒç´ åæ ‡
            color: é¢œè‰² (R, G, B)
            radius: ç‚¹çš„åŠå¾„
        """
        H, W = image.shape[:2]
        
        # ç»˜åˆ¶å°åœ†å½¢
        for du in range(-radius, radius + 1):
            for dv in range(-radius, radius + 1):
                if du*du + dv*dv <= radius*radius:
                    new_u, new_v = u + du, v + dv
                    if 0 <= new_u < W and 0 <= new_v < H:
                        # ä½¿ç”¨alphaæ··åˆ
                        alpha = 255
                        image[new_v, new_u] = [color[0], color[1], color[2], alpha]
    
    def generate_point_cloud(self, predictions: dict) -> Tuple[np.ndarray, np.ndarray]:
        """
        ç”Ÿæˆç‚¹äº‘åæ ‡å’Œé¢œè‰²
        
        Args:
            predictions: æ¨ç†ç»“æœ
            
        Returns:
            ç‚¹äº‘åæ ‡å’Œé¢œè‰²
        """
        if "world_points" not in predictions:
            raise ValueError("é¢„æµ‹ç»“æœä¸­æœªåŒ…å«world_points")
        
        # ä½¿ç”¨é¢„æµ‹çš„ç‚¹äº‘
        points_3d = predictions["world_points"]
        if torch.is_tensor(points_3d):
            points_3d = points_3d.cpu().numpy()
        
        # è·å–å›¾åƒæ•°æ®
        images_sample = predictions["images"][0]  # ç§»é™¤batchç»´åº¦
        
        # åŒè§†è§’æ¨¡å¼ï¼šimages_sampleå½¢çŠ¶ä¸º (2, 3, 294, 518)
        ext1_rgb = images_sample[0]  # (3, 294, 518)
        ext2_rgb = images_sample[1]  # (3, 294, 518)
        
        if torch.is_tensor(ext1_rgb):
            ext1_rgb = ext1_rgb.cpu().numpy()
        if torch.is_tensor(ext2_rgb):
            ext2_rgb = ext2_rgb.cpu().numpy()
        
        # è½¬ç½®ä¸º(H, W, C)æ ¼å¼
        ext1_rgb = np.transpose(ext1_rgb, (1, 2, 0))  # (294, 518, 3)
        ext2_rgb = np.transpose(ext2_rgb, (1, 2, 0))  # (294, 518, 3)
        
        # å°†ä¸¤ä¸ªç›¸æœºçš„RGBå›¾åƒåˆå¹¶ä¸ºä¸€ä¸ªå¤§çš„é¢œè‰²æ•°ç»„
        colors = np.concatenate([ext1_rgb.reshape(-1, 3), ext2_rgb.reshape(-1, 3)], axis=0)  # (2*294*518, 3)
        
        # ç¡®ä¿ç‚¹äº‘å’Œé¢œè‰²æ•°é‡åŒ¹é…
        points_3d = points_3d.reshape(-1, 3)
        assert len(points_3d) == len(colors), f"ç‚¹äº‘å’Œé¢œè‰²æ•°é‡ä¸åŒ¹é…: {len(points_3d)} vs {len(colors)}"
        
        return points_3d, colors
    
    def project_points_to_camera(self, points_3d: np.ndarray, colors: np.ndarray, 
                                extrinsic: np.ndarray, intrinsic: np.ndarray,
                                img_size: Tuple[int, int] = (1080, 1920)) -> np.ndarray:
        """
        å°†ç‚¹äº‘æŠ•å½±åˆ°ç›¸æœºè§†è§’
        
        Args:
            points_3d: 3Dç‚¹äº‘åæ ‡ (N, 3) - ä¸–ç•Œåæ ‡ç³»
            colors: ç‚¹äº‘é¢œè‰² (N, 3)
            extrinsic: å¤–å‚ (3, 4) - world2cameraå˜æ¢çŸ©é˜µ
            intrinsic: å†…å‚ (3, 3)
            img_size: è¾“å‡ºå›¾åƒå°ºå¯¸ (H, W) - é»˜è®¤(1080, 1920)
            
        Returns:
            æŠ•å½±å›¾åƒ (H, W, 4) - RGBAæ ¼å¼ï¼ŒèƒŒæ™¯é€æ˜
        """
        H, W = img_size
        
        # åˆ›å»ºç™½è‰²èƒŒæ™¯ (RGBA å…¨ 255)
        image = np.full((H, W, 4), 255, dtype=np.uint8)
        
        # å°†ç‚¹äº‘ä»ä¸–ç•Œåæ ‡è½¬æ¢åˆ°ç›¸æœºåæ ‡
        points_homo = np.concatenate([points_3d, np.ones((points_3d.shape[0], 1))], axis=1)  # (N, 4)
        points_cam = (extrinsic @ points_homo.T).T  # (N, 3)
        
        # è¿‡æ»¤æ‰ç›¸æœºåé¢çš„ç‚¹
        valid_mask = points_cam[:, 2] > 0 
        points_cam = points_cam[valid_mask]
        colors = colors[valid_mask]
        valid_mask = points_cam[:, 2] <1
        points_cam = points_cam[valid_mask]
        colors = colors[valid_mask]
        
        if len(points_cam) == 0:
            print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æŠ•å½±ç‚¹")
            return image
        
        # æŠ•å½±åˆ°å›¾åƒå¹³é¢
        points_2d = (intrinsic @ points_cam.T).T  # (N, 3)
        points_2d = points_2d[:, :2] / points_2d[:, 2:3]  # é€è§†é™¤æ³•
        
        # è¿‡æ»¤æ‰å›¾åƒè¾¹ç•Œå¤–çš„ç‚¹
        valid_mask = ((points_2d[:, 0] >= 0) & (points_2d[:, 0] < W) & 
                     (points_2d[:, 1] >= 0) & (points_2d[:, 1] < H))
        points_2d = points_2d[valid_mask]
        colors = colors[valid_mask]
        points_cam = points_cam[valid_mask]  # ä¿æŒç›¸æœºåæ ‡ç”¨äºæ·±åº¦æ’åº
        
        if len(points_2d) == 0:
            print("âš ï¸ æ²¡æœ‰æŠ•å½±åˆ°å›¾åƒå†…çš„ç‚¹")
            return image
        
        # æŒ‰ç…§æ·±åº¦ï¼ˆZåæ ‡ï¼‰æ’åºï¼Œå®ç°æ­£ç¡®çš„3Dé®æŒ¡å…³ç³»
        # æ·±åº¦å€¼è¶Šå°ï¼ˆè¶Šè¿‘ï¼‰çš„ç‚¹æ’åœ¨åé¢ï¼Œè¿™æ ·ä¼šè¢«å…ˆæ¸²æŸ“ï¼Œè¿‘å¤„çš„ç‚¹ä¼šè¦†ç›–è¿œå¤„çš„ç‚¹
        
        depth_values = points_cam[:, 2]  # Zåæ ‡ä½œä¸ºæ·±åº¦
        sort_indices = np.argsort(depth_values)[::-1]  # é™åºæ’åˆ—ï¼Œæ·±åº¦å¤§çš„ï¼ˆè¿œçš„ï¼‰å…ˆæ¸²æŸ“
        
        points_2d_sorted = points_2d[sort_indices]
        colors_sorted = colors[sort_indices]
        depth_sorted = depth_values[sort_indices]
        
        print(f"ğŸ” æ·±åº¦æ’åºä¿¡æ¯:")
        print(f"  æœ€è¿‘ç‚¹æ·±åº¦: {depth_sorted[-1]:.3f}")
        print(f"  æœ€è¿œç‚¹æ·±åº¦: {depth_sorted[0]:.3f}")
        print(f"  æ·±åº¦èŒƒå›´: {depth_sorted[0] - depth_sorted[-1]:.3f}")
        
        # å°†ç‚¹æŠ•å½±åˆ°å›¾åƒä¸Šï¼ˆæŒ‰æ·±åº¦æ’åºæ¸²æŸ“ï¼‰
        for i, (u, v) in enumerate(points_2d_sorted):
            u_int, v_int = int(round(u)), int(round(v))
            if 0 <= u_int < W and 0 <= v_int < H:
                # ç¡®ä¿é¢œè‰²å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
                color = np.clip(colors_sorted[i] * 255, 0, 255).astype(np.uint8)
                # æ¸²æŸ“ç‚¹ï¼ˆå¯ä»¥æ‰©å±•ä¸ºå°åœ†å½¢ä»¥æé«˜å¯è§æ€§ï¼‰
                self._render_point(image, u_int, v_int, color, radius=4)
        
        print(f"âœ… æŠ•å½±å®Œæˆï¼Œæœ‰æ•ˆç‚¹æ•°: {len(points_2d)}")
        return image
    
    def save_point_cloud_as_glb(self, points: np.ndarray, colors: np.ndarray, output_path: str):
        """
        å°†ç‚¹äº‘ä¿å­˜ä¸ºGLBæ ¼å¼
        
        Args:
            points: ç‚¹äº‘åæ ‡ (N, 3)
            colors: ç‚¹äº‘é¢œè‰² (N, 3)
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸ’¾ ä¿å­˜ç‚¹äº‘ä¸ºGLBæ ¼å¼: {output_path}")
        
        # ç¡®ä¿é¢œè‰²å€¼åœ¨æ­£ç¡®èŒƒå›´å†…
        if colors.max() <= 1.0:
            colors = (colors * 255).astype(np.uint8)
        else:
            colors = colors.astype(np.uint8)
        
        # åˆ›å»ºtrimeshç‚¹äº‘å¯¹è±¡
        point_cloud = trimesh.PointCloud(
            vertices=points,
            colors=colors
        )
        
        # å¯¼å‡ºä¸ºGLBæ ¼å¼
        point_cloud.export(output_path)
        print(f"âœ… GLBç‚¹äº‘ä¿å­˜å®Œæˆ: {output_path}")
    
    def add_wrist_origin_sphere(self, points_3d: np.ndarray, colors: np.ndarray, 
                               wrist_extrinsic: np.ndarray, radius: float = 0.05) -> Tuple[np.ndarray, np.ndarray]:
        """
        åœ¨ç‚¹äº‘ä¸­æ·»åŠ çº¢çƒè¡¨ç¤ºwrist origin
        
        Args:
            points_3d: åŸå§‹ç‚¹äº‘åæ ‡ (N, 3)
            colors: åŸå§‹ç‚¹äº‘é¢œè‰² (N, 3)
            wrist_extrinsic: wristç›¸æœºå¤–å‚çŸ©é˜µ (3, 4) - world2cameraå˜æ¢çŸ©é˜µ
            radius: çƒä½“åŠå¾„
            
        Returns:
            åŒ…å«çº¢çƒçš„ç‚¹äº‘åæ ‡å’Œé¢œè‰²
        """
        # æå–wrist originä½ç½®
        # wrist_extrinsicæ˜¯world2cameraå˜æ¢çŸ©é˜µT_wc
        # è¦å¾—åˆ°ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„ä½ç½®ï¼Œéœ€è¦æ±‚é€†ï¼šT_cw = inv(T_wc)
        # ç›¸æœºä½ç½® = T_cw * [0,0,0,1] = T_cwçš„å¹³ç§»éƒ¨åˆ†
        wrist_ext_4x4 = np.vstack([wrist_extrinsic, [0, 0, 0, 1]])  # æ‰©å±•ä¸º4x4é½æ¬¡åæ ‡çŸ©é˜µ
        wrist_ext_inv = np.linalg.inv(wrist_ext_4x4)  # æ±‚é€†å¾—åˆ°camera2worldå˜æ¢
        wrist_origin = wrist_ext_inv[:3, 3]  # å–é€†çŸ©é˜µçš„å¹³ç§»éƒ¨åˆ†
        
        # ç”Ÿæˆçº¢çƒç‚¹äº‘
        sphere_points, sphere_colors = self._generate_sphere_points(
            center=wrist_origin,
            radius=radius,
            color=(255, 0, 0),  # çº¢è‰²
            num_points=100
        )
        
        # åˆå¹¶åŸå§‹ç‚¹äº‘å’Œçº¢çƒ
        combined_points = np.vstack([points_3d, sphere_points])
        combined_colors = np.vstack([colors, sphere_colors])
        
        print(f"âœ… æ·»åŠ wrist originçº¢çƒï¼Œçƒå¿ƒä½ç½®: {wrist_origin}")
        return combined_points, combined_colors
    
    def _generate_sphere_points(self, center: np.ndarray, radius: float, 
                               color: Tuple[int, int, int], num_points: int = 100) -> Tuple[np.ndarray, np.ndarray]:
        """
        ç”Ÿæˆçƒä½“ç‚¹äº‘
        
        Args:
            center: çƒå¿ƒåæ ‡ (3,)
            radius: çƒåŠå¾„
            color: çƒé¢œè‰² (R, G, B)
            num_points: çƒä½“ç‚¹æ•°
            
        Returns:
            çƒä½“ç‚¹äº‘åæ ‡å’Œé¢œè‰²
        """
        # ç”Ÿæˆçƒé¢å‡åŒ€åˆ†å¸ƒçš„ç‚¹
        phi = np.linspace(0, 2*np.pi, int(np.sqrt(num_points)))
        theta = np.linspace(0, np.pi, int(np.sqrt(num_points)))
        phi, theta = np.meshgrid(phi, theta)
        
        # çƒåæ ‡è½¬ç¬›å¡å°”åæ ‡
        x = radius * np.sin(theta) * np.cos(phi)
        y = radius * np.sin(theta) * np.sin(phi)
        z = radius * np.cos(theta)
        
        # å±•å¹³å¹¶æ·»åŠ çƒå¿ƒåç§»
        sphere_points = np.concatenate([x.flatten(), y.flatten(), z.flatten()]).reshape(-1, 3)
        sphere_points = sphere_points + center  # ä½¿ç”¨numpyå¹¿æ’­æœºåˆ¶
        
        # ç”Ÿæˆé¢œè‰²
        sphere_colors = np.full((len(sphere_points), 3), color, dtype=np.uint8)
        
        return sphere_points, sphere_colors
    
    def visualize_point_cloud(self, ext1_video_path: str, ext2_video_path: str, 
                             output_path: str, save_glb: bool = True) -> dict:
        """
        å®Œæ•´çš„ç‚¹äº‘å¯è§†åŒ–æµç¨‹
        
        Args:
            ext1_video_path: ext1è§†é¢‘è·¯å¾„
            ext2_video_path: ext2è§†é¢‘è·¯å¾„
            output_path: è¾“å‡ºPNGè·¯å¾„
            save_glb: æ˜¯å¦ä¿å­˜GLBæ ¼å¼ç‚¹äº‘
            
        Returns:
            ç»“æœç»Ÿè®¡ä¿¡æ¯
        """
        print("ğŸ¯ å¼€å§‹ç‚¹äº‘å¯è§†åŒ–æµç¨‹...")
        # è®¡ç®—ä¸¤è·¯è§†é¢‘çš„æ€»å¸§æ•°ï¼Œå–æœ€å°ä»¥ä¿æŒåŒæ­¥
        total_frames_ext1 = self._get_video_frame_count(ext1_video_path)
        total_frames_ext2 = self._get_video_frame_count(ext2_video_path)
        total_frames = min(total_frames_ext1, total_frames_ext2)

        output_dir = Path("./pointcloud_vis")
        output_dir.mkdir(parents=True, exist_ok=True)

        for i in range(0, total_frames, 1):
            # 1. æå–é¦–å¸§
            frame1 = self.extract_first_frame(ext1_video_path,frame_num=i)
            frame2 = self.extract_first_frame(ext2_video_path,frame_num=i)
            
            # 2. è¿è¡ŒVGGTæ¨ç†
            predictions = self.run_inference(frame1, frame2)
            
            # 3. ç”Ÿæˆç‚¹äº‘
            points_3d, colors = self.generate_point_cloud(predictions)
            
            # 4. è·å–ç›¸æœºå‚æ•°
            ext1_extrinsic = predictions["extrinsic"][0, 0]  # (3, 4)
            ext2_extrinsic = predictions["extrinsic"][0, 1]  # (3, 4)
            ext1_intrinsic = predictions["intrinsic"][0, 0]  # (3, 3)
            
            # 5. çƒé¢æ’å€¼è®¡ç®—å¹³å‡å¤–å‚
            avg_extrinsic = self.spherical_interpolate_extrinsics(ext1_extrinsic, ext2_extrinsic, t=0.5)
            
            # 6. è°ƒæ•´å†…å‚ï¼šå¢å¤§ä¸€å€è§†é‡ï¼ˆç„¦è·å‡åŠï¼Œä¸»ç‚¹è°ƒæ•´åˆ°å›¾åƒä¸­å¿ƒï¼‰
            adjusted_intrinsic = self._adjust_intrinsic_for_larger_fov(ext1_intrinsic, target_size=(1080, 1920))
            
            # 7. æŠ•å½±åˆ°1920x1080å›¾åƒ
            projection_image = self.project_points_to_camera(
                points_3d, colors, ext1_extrinsic, adjusted_intrinsic, img_size=(1080, 1920)
            )
            
            # 8. å åŠ wristç›¸æœºå…‰é”¥ï¼ˆå¦‚æœæœ‰ï¼‰
            if "wrist_extrinsic" in predictions and "wrist_intrinsic" in predictions:
                wrist_extrinsic = predictions["wrist_extrinsic"][0, 0]  # (3,4)
                wrist_intrinsic = predictions["wrist_intrinsic"][0, 0]  # (3,3)
                projection_image = self._draw_wrist_frustum(
                    projection_image,
                    wrist_extrinsic,
                    wrist_intrinsic,
                    ext1_extrinsic,
                    adjusted_intrinsic,
                    img_size=(1080, 1920),
                    frustum_length=0.2,
                    color_bgra=(0, 255, 0, 255),
                    thickness=2,
                )
            
            # 9. ä¿å­˜æ¯å¸§å¯è§†åŒ–åˆ° ./pointcloud_vis/{frame_id}.png
            per_frame_path = output_dir / f"{i}.png"
            pil_image = Image.fromarray(projection_image, 'RGBA')
            pil_image.save(per_frame_path, 'PNG')
            print(f"âœ… ç¬¬{i}å¸§å¯è§†åŒ–å®Œæˆï¼Œä¿å­˜åˆ°: {per_frame_path}")
            
            # 10. ä¿å­˜GLBæ ¼å¼ç‚¹äº‘ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            glb_files = {}
            if save_glb:
                # ä¿å­˜åŸå§‹ç‚¹äº‘
                glb_output_dir = output_dir / "pointclouds"
                glb_output_dir.mkdir(exist_ok=True)
                
                # åŸå§‹ç‚¹äº‘GLB
                original_glb_path = glb_output_dir / f"frame_{i}_original.glb"
                self.save_point_cloud_as_glb(points_3d, colors, str(original_glb_path))
                glb_files["original_pointcloud"] = str(original_glb_path)
                
                # å¸¦wrist originçº¢çƒçš„ç‚¹äº‘GLBï¼ˆå¦‚æœæœ‰wrist poseé¢„æµ‹ï¼‰
                # if "wrist_extrinsic" in predictions:
                #     wrist_extrinsic = predictions["wrist_extrinsic"][0, 0]  # (3, 4)
                #     points_with_sphere, colors_with_sphere = self.add_wrist_origin_sphere(
                #         points_3d, colors, wrist_extrinsic
                #     )
                #     sphere_glb_path = glb_output_dir / f"{output_path.stem}_with_wrist_sphere_{i}.glb"
                #     self.save_point_cloud_as_glb(points_with_sphere, colors_with_sphere, str(sphere_glb_path))
                #     glb_files["pointcloud_with_wrist_sphere"] = str(sphere_glb_path)
                # else:
                #     print("âš ï¸  æœªæ£€æµ‹åˆ°wrist poseé¢„æµ‹ï¼Œè·³è¿‡wrist originçº¢çƒæ·»åŠ ")
            
            # 11. ç”Ÿæˆç»“æœç»Ÿè®¡
            results = {
                "input_videos": {
                    "ext1": ext1_video_path,
                    "ext2": ext2_video_path
                },
                "output_file": str(per_frame_path),
                "glb_files": glb_files,
                "statistics": {
                    "total_points": len(points_3d),
                    "valid_projection_points": len([p for p in points_3d if p[2] > 0]),
                    "image_resolution": "1920x1080",
                    "background_transparent": False,
                    "extrinsic_interpolation": "spherical_slerp",
                    "intrinsic_source": "ext1",
                    "glb_saved": save_glb
                }
            }
            
        return results


def main():
    parser = argparse.ArgumentParser(description="VGGTç‚¹äº‘å¯è§†åŒ–è„šæœ¬")
    parser.add_argument("--checkpoint", required=True, help="VGGT checkpointè·¯å¾„")
    parser.add_argument("--ext1", required=True, help="ext1è§†é¢‘è·¯å¾„")
    parser.add_argument("--ext2", required=True, help="ext2è§†é¢‘è·¯å¾„")
    parser.add_argument("--output", required=True, help="è¾“å‡ºPNGè·¯å¾„")
    parser.add_argument("--device", default="cuda", help="æ¨ç†è®¾å¤‡ (cuda/cpu)")
    parser.add_argument("--save-glb", action="store_true", default=True, help="æ˜¯å¦ä¿å­˜GLBæ ¼å¼ç‚¹äº‘ï¼ˆé»˜è®¤ï¼šTrueï¼‰")
    parser.add_argument("--no-glb", action="store_true", help="ç¦ç”¨GLBä¿å­˜")
    
    args = parser.parse_args()
    
    # å¤„ç†GLBä¿å­˜é€‰é¡¹
    save_glb = args.save_glb and not args.no_glb
    
    # éªŒè¯è¾“å…¥æ–‡ä»¶
    for video_path, name in [(args.ext1, "ext1"), (args.ext2, "ext2")]:
        if not os.path.exists(video_path):
            print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            return
    
    if not os.path.exists(args.checkpoint):
        print(f"âŒ Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {args.checkpoint}")
        return
    
    try:
        # åˆ›å»ºå¯è§†åŒ–å™¨
        visualizer = VGGTPointCloudVisualizer(args.checkpoint, args.device)
        
        # è¿è¡Œå¯è§†åŒ–
        results = visualizer.visualize_point_cloud(args.ext1, args.ext2, args.output, save_glb=save_glb)
        
        print("\n" + "="*80)
        print("ğŸ“‹ ç‚¹äº‘å¯è§†åŒ–ç»“æœæ‘˜è¦:")
        print(f"  è¾“å…¥è§†é¢‘: {args.ext1}, {args.ext2}")
        print(f"  æ¨¡å‹checkpoint: {args.checkpoint}")
        print(f"  è¾“å‡ºæ–‡ä»¶: {args.output}")
        print(f"  æ€»ç‚¹æ•°: {results['statistics']['total_points']}")
        print(f"  æœ‰æ•ˆæŠ•å½±ç‚¹æ•°: {results['statistics']['valid_projection_points']}")
        print(f"  å›¾åƒåˆ†è¾¨ç‡: {results['statistics']['image_resolution']}")
        print(f"  èƒŒæ™¯é€æ˜: {results['statistics']['background_transparent']}")
        print(f"  å¤–å‚æ’å€¼: {results['statistics']['extrinsic_interpolation']}")
        print(f"  å†…å‚æ¥æº: {results['statistics']['intrinsic_source']}")
        print(f"  GLBä¿å­˜: {results['statistics']['glb_saved']}")
        
        if results['glb_files']:
            print("\nğŸ“¦ GLBæ–‡ä»¶:")
            for name, path in results['glb_files'].items():
                print(f"  {name}: {path}")
        
        print("="*80)
        
    except Exception as e:
        print(f"âŒ ç‚¹äº‘å¯è§†åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
