#!/usr/bin/env python3
"""
VGGT è®­ç»ƒéªŒè¯å¯è§†åŒ–å·¥å…·

æä¾›è®­ç»ƒè¿‡ç¨‹ä¸­çš„éªŒè¯å¯è§†åŒ–åŠŸèƒ½ï¼š
- æ·±åº¦å›¾å¯è§†åŒ–ï¼ˆext1å’Œext2åˆ†åˆ«ï¼‰
- ç‚¹äº‘ç”Ÿæˆå’Œä¿å­˜ï¼ˆå«wrist originçº¢çƒï¼‰
- wristè§†è§’ç‚¹äº‘æŠ•å½±
"""

import os
from re import S
import sys
import numpy as np
import cv2
from PIL import Image, ImageDraw
import torch
import torch.nn.functional as F
import trimesh
import matplotlib.pyplot as plt
from pathlib import Path
import json
from typing import Tuple, List, Optional, Dict, Any
import logging
from datetime import datetime
from vggt.utils.pose_enc import extri_intri_to_pose_encoding,pose_encoding_to_extri_intri



class ValidationVisualizer:
    """
    éªŒè¯é˜¶æ®µçš„å¯è§†åŒ–å·¥å…·
    """
    
    def __init__(self, output_base_dir: str = "logs", rank: int = 0, experiment_name: str = None):
        """
        åˆå§‹åŒ–å¯è§†åŒ–å·¥å…·
        
        Args:
            output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
            rank: åˆ†å¸ƒå¼è®­ç»ƒçš„rank
        """
        self.output_base_dir = Path(output_base_dir)
        self.rank = rank
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åªåœ¨rank 0ä¸Šè¿›è¡Œå¯è§†åŒ–ï¼Œé¿å…å¤šè¿›ç¨‹å†²çª
        self.should_visualize = (rank == 0)
        
        # ğŸ”§ ä¿®å¤ï¼šå§‹ç»ˆåˆå§‹åŒ–ç›®å½•è·¯å¾„ï¼Œé¿å…å±æ€§ç¼ºå¤±é”™è¯¯
        # ğŸ†• ç”Ÿæˆæ—¶é—´æˆ³ç›®å½•ï¼Œæ¯æ¬¡è®­ç»ƒä¼šè¯ä½¿ç”¨ç‹¬ç«‹ç›®å½•
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        if experiment_name is None:
            self.val_vis_base = self.output_base_dir / "validation_visualizations"
        else:
            self.val_vis_base = self.output_base_dir / "validation_visualizations" / experiment_name
        self.val_vis_dir = self.val_vis_base / timestamp
        # self.depth_dir = self.val_vis_dir / "depth_maps"
        # self.pointcloud_dir = self.val_vis_dir / "pointclouds"
        # self.gt_pointcloud_dir = self.val_vis_dir / "gt_pointclouds"  # ğŸ†• GTç‚¹äº‘ç›®å½•
        self.projection_dir = self.val_vis_dir
        if self.should_visualize:
            # åˆ›å»ºæ‰€æœ‰å¿…è¦ç›®å½•
            for dir_path in [self.val_vis_dir]:
                dir_path.mkdir(parents=True, exist_ok=True)
                
            logging.info(f"âœ… éªŒè¯å¯è§†åŒ–ç›®å½•å·²åˆ›å»º: {self.val_vis_dir}")
            logging.info(f"ğŸ“… æ—¶é—´æˆ³ä¼šè¯ç›®å½•: {timestamp}")
        
        logging.info(f"ğŸ¨ ValidationVisualizer åˆå§‹åŒ–å®Œæˆ (rank={rank}, visualize={self.should_visualize})")
    
    def visualize_validation_results(self, predictions: Dict, batch: Dict, epoch: int, batch_idx: int) -> Dict[str, str]:
        """
        å®Œæ•´çš„éªŒè¯ç»“æœå¯è§†åŒ–
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœ
            batch: è¾“å…¥batchæ•°æ®
            epoch: å½“å‰epoch
            batch_idx: å½“å‰batchç´¢å¼•
            
        Returns:
            ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        import cv2
        import json
        from pathlib import Path
        import gc
        
        # === å†…å­˜ç®¡ç†ï¼šå¼€å§‹å‰æ¸…ç† ===
        gc.collect()
        if hasattr(torch, 'cuda') and torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # === ç”±äºbatch_sizeæ˜¯24ï¼Œåªä¿ç•™ç¬¬0ä¸ªsampleè¿›è¡Œå¯è§†åŒ– ===
        # å¯¹æ‰€æœ‰batchæ•°æ®è¿›è¡Œåˆ‡ç‰‡ï¼Œåªä¿ç•™ç¬¬0ä¸ªæ ·æœ¬
        for key in batch:
            if isinstance(batch[key], np.ndarray) and batch[key].ndim > 0:
                batch[key] = batch[key][:1]  # åªä¿ç•™ç¬¬0ä¸ªæ ·æœ¬
            if isinstance(batch[key], torch.Tensor) and batch[key].ndim > 0:
                batch[key] = batch[key][:1]  # åªä¿ç•™ç¬¬0ä¸ªæ ·æœ¬
            elif isinstance(batch[key], list) and len(batch[key]) > 0:
                batch[key] = batch[key][:1]  # åªä¿ç•™ç¬¬0ä¸ªæ ·æœ¬
        
        # === ä¸¥æ ¼çš„æ–­è¨€æ£€æŸ¥ - ç¡®ä¿å”¯ä¸€å½¢çŠ¶ ===
        # 1. æ£€æŸ¥batchåŸºæœ¬ç»“æ„
        assert "images" in batch, "batchä¸­ç¼ºå°‘images"
        # assert "depths" in batch, "batchä¸­ç¼ºå°‘depths"
        # assert "depth_nan_masks" in batch, "batchä¸­ç¼ºå°‘depth_nan_masks"
        assert "extrinsics" in batch, "batchä¸­ç¼ºå°‘extrinsics"
        assert "intrinsics" in batch, "batchä¸­ç¼ºå°‘intrinsics"
        # assert "point_cloud" in batch, "batchä¸­ç¼ºå°‘point_cloud"
        # assert "point_colors" in batch, "batchä¸­ç¼ºå°‘point_colors"
        
        # 2. æ£€æŸ¥å›¾åƒæ•°æ®å½¢çŠ¶ - å”¯ä¸€å½¢çŠ¶
        images = batch["images"]
        # depths = batch["depths"]
        # depth_nan_masks = batch["depth_nan_masks"]
        
        # æ£€æŸ¥batchç»´åº¦ï¼ˆç°åœ¨åº”è¯¥æ˜¯å•æ ·æœ¬ï¼‰
        assert images.ndim == 5, f"å›¾åƒç»´åº¦é”™è¯¯: {images.ndim}ï¼ŒæœŸæœ›5 (B,S,C,H,W)"
        # assert depths.ndim == 4, f"æ·±åº¦å›¾ç»´åº¦é”™è¯¯: {depths.ndim}ï¼ŒæœŸæœ›4 (B,S,H,W)"
        # assert depth_nan_masks.ndim == 4, f"æ·±åº¦maskç»´åº¦é”™è¯¯: {depth_nan_masks.ndim}ï¼ŒæœŸæœ›4 (B,S,H,W)"
        
        # æ£€æŸ¥batch sizeï¼ˆç°åœ¨åº”è¯¥æ˜¯1ï¼‰
        assert images.shape[0] == 1, f"æœŸæœ›batch sizeä¸º1ï¼Œå®é™…{images.shape[0]}"
        # assert depths.shape[0] == 1, f"æœŸæœ›batch sizeä¸º1ï¼Œå®é™…{depths.shape[0]}"
        # assert depth_nan_masks.shape[0] == 1, f"æœŸæœ›batch sizeä¸º1ï¼Œå®é™…{depth_nan_masks.shape[0]}"
        
        # æ£€æŸ¥åºåˆ—é•¿åº¦ï¼ˆç›¸æœºæ•°é‡ï¼‰
        # assert images.shape[1] == 2, f"æœŸæœ›2ä¸ªç›¸æœºï¼Œå®é™…{images.shape[1]}ä¸ª"
        # assert depths.shape[1] == 2, f"æœŸæœ›2ä¸ªæ·±åº¦å›¾ï¼Œå®é™…{depths.shape[1]}ä¸ª"
        # assert depth_nan_masks.shape[1] == 2, f"æœŸæœ›2ä¸ªæ·±åº¦maskï¼Œå®é™…{depth_nan_masks.shape[1]}ä¸ª"
        
        # æ£€æŸ¥å›¾åƒå°ºå¯¸ - æ³¨æ„å›¾åƒæ˜¯(B,S,C,H,W)æ ¼å¼
        assert images.shape[2] == 3, f"å›¾åƒå½¢çŠ¶é”™è¯¯: {images.shape}ï¼ŒæœŸæœ›3channel"
        # assert depths.shape[2:] == (294, 518), f"æ·±åº¦å›¾å½¢çŠ¶é”™è¯¯: {depths.shape[2:]}ï¼ŒæœŸæœ›(294,518)"
        # assert depth_nan_masks.shape[2:] == (294, 518), f"æ·±åº¦maskå½¢çŠ¶é”™è¯¯: {depth_nan_masks.shape[2:]}ï¼ŒæœŸæœ›(294,518)"
        
        # 3. æ£€æŸ¥ç›¸æœºå‚æ•°å½¢çŠ¶ - å”¯ä¸€å½¢çŠ¶ï¼ˆæ”¯æŒä»»æ„è§†è§’æ•°Sï¼‰
        extrinsics = batch["extrinsics"]  # (1, S, 3, 4)
        intrinsics = batch["intrinsics"]  # (1, S, 3, 3)
        wrist_extrinsics = batch["wrist_extrinsics"]  # (1, 3, 4)
        wrist_intrinsics = batch["wrist_intrinsics"]  # (1, 3, 3)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆå¦‚æœæ˜¯tensorï¼‰
        if torch.is_tensor(extrinsics):
            extrinsics = extrinsics.cpu().numpy()
        if torch.is_tensor(intrinsics):
            intrinsics = intrinsics.cpu().numpy()
        if torch.is_tensor(wrist_extrinsics):
            wrist_extrinsics = wrist_extrinsics.cpu().numpy()
        if torch.is_tensor(wrist_intrinsics):
            wrist_intrinsics = wrist_intrinsics.cpu().numpy()
        
        wrist_extrinsics = wrist_extrinsics[0]
        wrist_intrinsics = wrist_intrinsics[0]
        assert extrinsics.ndim == 4 and extrinsics.shape[0] == 1 and extrinsics.shape[2:] == (3, 4), f"wristå¤–å‚å½¢çŠ¶é”™è¯¯: {extrinsics.shape}ï¼ŒæœŸæœ›(1,S,3,4)"
        assert intrinsics.ndim == 4 and intrinsics.shape[0] == 1 and intrinsics.shape[2:] == (3, 3), f"wristå†…å‚å½¢çŠ¶é”™è¯¯: {intrinsics.shape}ï¼ŒæœŸæœ›(1,S,3,3)"
        assert wrist_extrinsics.shape == (1, 3, 4), f"wristå¤–å‚å½¢çŠ¶é”™è¯¯: {wrist_extrinsics.shape}ï¼ŒæœŸæœ›(1,3,4)"
        assert wrist_intrinsics.shape == (1, 3, 3), f"wristå†…å‚å½¢çŠ¶é”™è¯¯: {wrist_intrinsics.shape}ï¼ŒæœŸæœ›(1,3,3)"
        
        # 4. æ£€æŸ¥ç‚¹äº‘æ•°æ® - å”¯ä¸€å½¢çŠ¶
        # ä½¿ç”¨é¢„æµ‹çš„ç‚¹äº‘è€Œä¸æ˜¯GTç‚¹äº‘
        if "world_points" in predictions:
            points_3d = predictions["world_points"][0]
            if torch.is_tensor(points_3d):
                points_3d = points_3d.cpu().numpy()
        else:
            raise ValueError("predictions ç¼ºå°‘ world_points")
        points_3d = points_3d.reshape(-1, 3)  # é‡å¡‘ä¸º(N, 3)
        
        # è·å–å•ä¸ªæ ·æœ¬çš„å›¾åƒæ•°æ®
        images_sample = images[0]
        
        # ç»Ÿä¸€ç”Ÿæˆé¢œè‰²ï¼šæ‹¼æ¥æ‰€æœ‰è§†è§’RGB
        colors_list = []
        for cam_idx in range(images_sample.shape[0]):
            rgb = images_sample[cam_idx]
            if torch.is_tensor(rgb):
                rgb = rgb.cpu().numpy()
            rgb = np.transpose(rgb, (1, 2, 0))
            assert rgb.shape[-1] == 3, f"RGBå½¢çŠ¶é”™è¯¯: {rgb.shape}"
            colors_list.append(rgb.reshape(-1, 3))
        colors = np.concatenate(colors_list, axis=0)
        
        # ç¡®ä¿ç‚¹äº‘å’Œé¢œè‰²æ•°é‡åŒ¹é…
        assert len(points_3d) == len(colors), f"ç‚¹äº‘å’Œé¢œè‰²æ•°é‡ä¸åŒ¹é…: {len(points_3d)} vs {len(colors)}"
        assert points_3d.ndim == 2, f"ç‚¹äº‘ç»´åº¦é”™è¯¯: {points_3d.ndim}ï¼ŒæœŸæœ›2"
        assert points_3d.shape[1] == 3, f"ç‚¹äº‘å½¢çŠ¶é”™è¯¯: {points_3d.shape}ï¼ŒæœŸæœ›(N,3)"
        assert colors.ndim == 2, f"é¢œè‰²ç»´åº¦é”™è¯¯: {colors.ndim}ï¼ŒæœŸæœ›2"
        assert colors.shape[1] == 3, f"é¢œè‰²å½¢çŠ¶é”™è¯¯: {colors.shape}ï¼ŒæœŸæœ›(N,3)"
        
        # 5. æ£€æŸ¥predictionsåŸºæœ¬ç»“æ„
        assert "pose_enc" in predictions, "predictionsä¸­ç¼ºå°‘pose_enc"
        assert "wrist_pose_enc" in predictions, "predictionsä¸­ç¼ºå°‘wrist_pose_enc"
        
        
        # === åˆ›å»ºè¾“å‡ºç›®å½• ===
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        output_dir = Path(self.projection_dir) / f"epoch_{epoch}_batch_{batch_idx}_{timestamp}"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # === å¯è§†åŒ–å¤„ç† ===
        # ä½¿ç”¨ç¬¬0ä¸ªæ ·æœ¬ï¼ˆç°åœ¨batchä¸­åªæœ‰1ä¸ªæ ·æœ¬ï¼‰
        sample_idx = 0
        
        # æå–ç¬¬0ä¸ªæ ·æœ¬çš„æ•°æ®
        images_sample = images[sample_idx]
        # depths_sample = depths[sample_idx]
        # depth_nan_masks_sample = depth_nan_masks[sample_idx]
        extrinsics_sample = extrinsics[sample_idx]
        intrinsics_sample = intrinsics[sample_idx]
        
        # === 1. å¯è§†åŒ–ç›¸æœºè§†è§’ï¼ˆä»»æ„Sï¼‰ ===
        S = images_sample.shape[0]
        camera_names = [f"ext{i+1}" for i in range(S)]
        camera_indices = list(range(S))
        
        for i, (camera_name, camera_idx) in enumerate(zip(camera_names, camera_indices)):
            # è·å–å½“å‰ç›¸æœºçš„æ•°æ®
            camera_rgb = images_sample[camera_idx]  # (3, 294, 518)
            # camera_depth = depths_sample[camera_idx]  # (294, 518) - GT depth
            # camera_valid_depth = depth_nan_masks_sample[camera_idx]  # (294, 518)
            camera_extrinsic = extrinsics_sample[camera_idx]  # (3, 4)
            camera_intrinsic = intrinsics_sample[camera_idx]  # (3, 3)
            
            # è·å–é¢„æµ‹çš„depth
            assert "depth" in predictions, "predictionsä¸­ç¼ºå°‘depth"
            pred_depth = predictions["depth"][0, camera_idx].cpu().numpy()  # å–ç¬¬0ä¸ªæ ·æœ¬ï¼Œç¬¬camera_idxä¸ªç›¸æœº
            pred_depth = pred_depth.squeeze()  # ç§»é™¤æœ€åçš„ç»´åº¦ï¼Œä»(294, 518, 1)å˜ä¸º(294, 518)
            
            # è½¬æ¢ä¸ºnumpyå¹¶å¤„ç†ç»´åº¦
            if torch.is_tensor(camera_rgb):
                camera_rgb = camera_rgb.cpu().numpy()
            # if torch.is_tensor(camera_depth):
            #     camera_depth = camera_depth.cpu().numpy()
            # if torch.is_tensor(camera_valid_depth):
            #     camera_valid_depth = camera_valid_depth.cpu().numpy()
            if torch.is_tensor(camera_extrinsic):
                camera_extrinsic = camera_extrinsic.cpu().numpy()
            if torch.is_tensor(camera_intrinsic):
                camera_intrinsic = camera_intrinsic.cpu().numpy()
             
            # è½¬æ¢RGBæ ¼å¼ (C, H, W) -> (H, W, C)
            camera_rgb = np.transpose(camera_rgb, (1, 2, 0))  # (294, 518, 3)
            camera_rgb = (camera_rgb * 255).astype(np.uint8)
            all_camera_rgb = images_sample.cpu().numpy()
            all_camera_rgb = np.transpose(all_camera_rgb, (0,2,3,1))
            all_camera_rgb = (all_camera_rgb*255).astype(np.uint8)
            # åˆ›å»º2x2ç½‘æ ¼
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
            fig.suptitle(f'{camera_name.upper()} Visualization', fontsize=16)
            
            # 1. RGBå›¾åƒ
            axes[0, 0].imshow(camera_rgb)
            axes[0, 0].set_title('RGB Image')
            axes[0, 0].axis('off')
            
            # 2. Prediction Depth - ä½¿ç”¨é¢„æµ‹çš„depth
            depth_img = axes[0, 1].imshow(pred_depth, cmap='viridis')
            axes[0, 1].set_title('Prediction Depth')
            axes[0, 1].axis('off')
            plt.colorbar(depth_img, ax=axes[0, 1], fraction=0.046, pad=0.04)
            
            # 3. GT Depth (ä½¿ç”¨valid mask)
            # gt_depth_masked = camera_depth.copy()
            # gt_depth_masked[~camera_valid_depth] = np.nan
            # gt_depth_img = axes[1, 0].imshow(gt_depth_masked, cmap='viridis')
            # axes[1, 0].set_title('GT Depth (Valid)')
            # axes[1, 0].axis('off')
            # plt.colorbar(gt_depth_img, ax=axes[1, 0], fraction=0.046, pad=0.04)
            
            # 4. Point Cloud Projection - ä½¿ç”¨é¢„æµ‹ç‚¹äº‘å’Œé¢„æµ‹ç›¸æœºå‚æ•°
            # è·å–é¢„æµ‹çš„ç‚¹äº‘ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            assert "world_points" in predictions
            pred_points_3d = predictions["world_points"][0].cpu().numpy()  # å–ç¬¬0ä¸ªæ ·æœ¬
            
            # ç»Ÿä¸€å¤„ç†world_pointsç»´åº¦
            if pred_points_3d.ndim == 3:
                pred_points_3d = pred_points_3d[None, ...]
            assert pred_points_3d.ndim == 4, f"Unexpected world_points shape: {pred_points_3d.shape}"
            pred_points_3d = pred_points_3d[camera_idx].reshape(-1, 3)
            pred_colors = all_camera_rgb[camera_idx].reshape(-1, 3)
            
            # ç¡®ä¿ç‚¹äº‘å’Œé¢œè‰²æ•°é‡åŒ¹é…
            assert len(pred_points_3d) == len(pred_colors), f"ç‚¹äº‘å’Œé¢œè‰²æ•°é‡ä¸åŒ¹é…: {len(pred_points_3d)} vs {len(pred_colors)}"
            
            # è·å–é¢„æµ‹çš„ç›¸æœºå‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            assert "pose_enc_list" in predictions
            # pose_encå½¢çŠ¶æ˜¯[24, 2, 9]ï¼Œå…¶ä¸­2è¡¨ç¤ºä¸¤ä¸ªç›¸æœº(ext1, ext2)
            # æˆ‘ä»¬éœ€è¦æ ¹æ®å½“å‰ç›¸æœºç´¢å¼•camera_idxæ¥è·å–å¯¹åº”çš„pose
            pred_pose_enc = predictions["pose_enc_list"][ -1][0, camera_idx].cpu()  # å–ç¬¬0ä¸ªæ ·æœ¬ï¼Œç¬¬camera_idxä¸ªç›¸æœº
            extrinsics,intrinsics = pose_encoding_to_extri_intri(pred_pose_enc.unsqueeze(0).unsqueeze(0),image_size_hw=camera_rgb.shape[:2])
            pred_extrinsic = extrinsics[0,0].numpy()
            pred_intrinsic = intrinsics[0,0].numpy()
            
            
            # æŠ•å½±ç‚¹äº‘
 
            projection = self.visualize_point_cloud_projection(
                points_3d=pred_points_3d,
                point_colors=pred_colors,
                camera_intrinsics=pred_intrinsic,
                camera_extrinsics=pred_extrinsic,
                image_shape=camera_rgb.shape[:2],
                need_inverse=False # å¯¹äºext1/ext2ï¼Œæ˜¯world2cameraï¼Œä¸éœ€è¦æ±‚é€†
            )
            axes[1, 1].imshow(projection)
            axes[1, 1].set_title('Point Cloud Projection')
            axes[1, 1].axis('off')
            
            # ä¿å­˜2x2ç½‘æ ¼
            output_path = output_dir / f"{camera_name}_grid.png"
            plt.tight_layout()
            plt.savefig(str(output_path), dpi=150, bbox_inches='tight')
            plt.close()
        
        # ä¿å­˜å•ç‹¬çš„projection
        
        # ä¿å­˜å•ç‹¬çš„RGBå’Œdepth
        
        # === 2. å¯è§†åŒ–GT wrist RGB ===
        if "wrist_image" in batch and batch["wrist_image"] is not None:
            wrist_rgb = batch["wrist_image"]
            
            # å¤„ç†batchç»´åº¦
            if wrist_rgb.ndim == 4:  # (B, H, W, 3) æ ¼å¼
                wrist_rgb = wrist_rgb[sample_idx]  # å–ç¬¬0ä¸ªæ ·æœ¬
            elif wrist_rgb.ndim == 3:  # (H, W, 3) æ ¼å¼ï¼Œå·²ç»æ˜¯å•ä¸ªæ ·æœ¬
                pass
        else:
            raise ValueError(f"wrist_rgbç»´åº¦é”™è¯¯: {wrist_rgb.ndim}ï¼ŒæœŸæœ›3æˆ–4")
        
        wrist_rgb = np.array(wrist_rgb.cpu())
        assert wrist_rgb.ndim == 3, f"wrist_rgbç»´åº¦é”™è¯¯: {wrist_rgb.ndim}ï¼ŒæœŸæœ›3"
        assert wrist_rgb.shape[2] == 3, f"wrist_rgbå½¢çŠ¶é”™è¯¯: {wrist_rgb.shape}ï¼ŒæœŸæœ›(H,W,3)"
        
        # ç¡®ä¿é¢œè‰²èŒƒå›´æ­£ç¡®
        wrist_rgb = (wrist_rgb).astype(np.uint8)
        
        # ä¿å­˜çœŸå®wrist RGBå›¾åƒ
        wrist_rgb_path = output_dir / "wrist_rgb.png"
        plt.imsave(str(wrist_rgb_path), wrist_rgb)  # ä½¿ç”¨plt.imsaveä¿æŒRGBæ ¼å¼
        
        # === 3.1. å¯è§†åŒ–wristæŠ•å½± ===
        # è·å–é¢„æµ‹çš„wrist poseå’Œç‚¹äº‘
        if "wrist_pose_enc_list" in predictions and "world_points" in predictions:
            # è·å–é¢„æµ‹çš„wrist pose
            wrist_pose_enc = predictions["wrist_pose_enc_list"][-1][0].cpu()  # å–ç¬¬0ä¸ªæ ·æœ¬
            # ğŸ”¥ NEW: wrist_head now outputs single wrist pose [B, 1, target_dim] instead of [B, S, target_dim]
            wrist_pose_enc = wrist_pose_enc[0]  # å–ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿæ˜¯å”¯ä¸€çš„ï¼‰wrist pose
            wrist_ext,wrist_intrinsics = pose_encoding_to_extri_intri(wrist_pose_enc.unsqueeze(0).unsqueeze(0),image_size_hw=wrist_rgb.shape[:2]) # camera2world
            wrist_ext = wrist_ext[0,0].numpy()
            # ä½¿ç”¨GT wrist intrinsicsè€Œä¸æ˜¯é¢„æµ‹çš„intrinsics
            wrist_intrinsics_gt = batch["wrist_intrinsics"][0].cpu().numpy()  # GT intrinsic
            # è·å–é¢„æµ‹çš„ç‚¹äº‘
            pred_points_3d = predictions["world_points"][0].cpu().numpy()
            pred_points_3d = pred_points_3d.reshape(-1, 3)  
            
            # ç¡®ä¿ç‚¹äº‘å’Œé¢œè‰²æ•°é‡åŒ¹é…
            if len(pred_points_3d) != len(colors):
                logging.warning(f"Wrist projection: ç‚¹äº‘å’Œé¢œè‰²æ•°é‡ä¸åŒ¹é…: {len(pred_points_3d)} vs {len(colors)}")
                # å¦‚æœæ•°é‡ä¸åŒ¹é…ï¼Œæˆªå–åˆ°è¾ƒå°çš„æ•°é‡
                min_count = min(len(pred_points_3d), len(colors))
                pred_points_3d = pred_points_3d[:min_count]
                colors = colors[:min_count]
            
            # æŠ•å½±åˆ°wristè§†è§’
            wrist_projection = self.visualize_point_cloud_projection(
                points_3d=pred_points_3d,
                point_colors=colors,  # ä½¿ç”¨åŸå§‹ç‚¹äº‘é¢œè‰²
                camera_extrinsics= wrist_ext,
                camera_intrinsics= wrist_intrinsics_gt[0],  # ä½¿ç”¨GT intrinsic
                image_shape=wrist_rgb.shape[:2],
                need_inverse=False # å¯¹äºwristï¼Œæ˜¯world2cameraï¼Œä¸éœ€è¦æ±‚é€†
            )
            
            # ä¿å­˜wristæŠ•å½±
            wrist_projection_path = output_dir / "wrist_projection.png"
            cv2.imwrite(str(wrist_projection_path), cv2.cvtColor(wrist_projection, cv2.COLOR_RGB2BGR))
            # åˆ›å»ºwristå¯¹æ¯”å›¾ï¼ˆåŸå›¾vsæŠ•å½±ï¼‰
            fig, axes = plt.subplots(1, 2, figsize=(12, 6))
            fig.suptitle('Wrist Camera: Original vs Projection', fontsize=16)
            
            axes[0].imshow(wrist_rgb)
            axes[0].set_title('Original Wrist RGB')
            axes[0].axis('off')
            
            axes[1].imshow(wrist_projection)
            axes[1].set_title('Point Cloud Projection')
            axes[1].axis('off')
            
            wrist_comparison_path = output_dir / "wrist_comparison.png"
            plt.tight_layout()
            plt.savefig(str(wrist_comparison_path), dpi=150, bbox_inches='tight')
            plt.close()
        
        # === 4. å¯è§†åŒ–ç‚¹äº‘ï¼ˆå¸¦çº¢çƒå’Œç»¿çƒï¼‰ ===
        # åˆ›å»ºå¸¦çº¢çƒçš„ç‚¹äº‘ï¼ˆé¢„æµ‹wrist poseï¼‰
        points_with_red_sphere = self._add_wrist_origin_sphere(
            predictions=predictions,  # ä¼ å…¥predictionsè€Œä¸æ˜¯batch
            points_3d=points_3d,
            colors=colors
        )
        
        # ä¿å­˜å¸¦çº¢çƒçš„ç‚¹äº‘
        red_sphere_path = output_dir / "pointcloud_with_red_sphere.glb"
        self._save_point_cloud_as_glb(
            points=points_with_red_sphere["points"],
            colors=points_with_red_sphere["colors"],
            output_path=str(red_sphere_path)
        )
        
        # === 5. ä¿å­˜å…ƒæ•°æ®ï¼ˆç»Ÿä¸€ï¼Œä»»æ„è§†è§’ï¼‰ ===
        metadata = {
            "epoch": epoch,
            "batch_idx": batch_idx,
            "timestamp": timestamp,
            "point_cloud_size": len(points_3d),
            "training_mode": f"multi_view_{images_sample.shape[0]}",
            "cameras": [f"ext{i+1}" for i in range(images_sample.shape[0])] + ["wrist"],
            "image_shapes": {f"ext{i+1}": images_sample[i].shape for i in range(images_sample.shape[0])},
        }
        
        metadata_path = output_dir / "metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # === å†…å­˜ç®¡ç†ï¼šç»“æŸåæ¸…ç† ===
        plt.close('all')
        gc.collect()
        if hasattr(torch, 'cuda') and torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # è¿”å›ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„ï¼ˆç»Ÿä¸€ï¼‰
        result = {
            "output_dir": str(output_dir),
            "metadata": str(metadata_path),
            "wrist_rgb": str(output_dir / "wrist_rgb.png"),
            "pointcloud_with_red_sphere": str(red_sphere_path),
        }
        for i in range(images_sample.shape[0]):
            name = f"ext{i+1}"
            result[f"{name}_grid"] = str(output_dir / f"{name}_grid.png")
            result[f"{name}_rgb"] = str(output_dir / f"{name}_rgb.png")
            result[f"{name}_depth"] = str(output_dir / f"{name}_depth.png")
            result[f"{name}_projection"] = str(output_dir / f"{name}_projection.png")
        
        # å¦‚æœæœ‰wristæŠ•å½±ï¼Œæ·»åŠ ç›¸å…³è·¯å¾„
        if "wrist_pose_enc" in predictions and "world_points" in predictions:
            result["wrist_projection"] = str(output_dir / "wrist_projection.png")
            result["wrist_comparison"] = str(output_dir / "wrist_comparison.png")
        
        # å¦‚æœæœ‰GT wrist poseï¼Œæ·»åŠ ç»¿çƒç‚¹äº‘è·¯å¾„
        # if "wrist_extrinsics" in batch and batch["wrist_extrinsics"] is not None:
            # result["gt_pointcloud_with_green_sphere"] = str(green_sphere_path)
        
        # === 6. æ–°å¢ï¼šProjectionå¯è§†åŒ– ===
        if "track_pairs" in batch and "wrist_pose_enc_list" in predictions and "world_points" in predictions:
 
            projection_vis_result = self._visualize_projection_tracking(
                predictions=predictions,
                batch=batch,
                output_dir=output_dir,
            )
            result.update(projection_vis_result)
         
        return result
    
    def _pose_to_extrinsics(self, pose_enc: np.ndarray) -> np.ndarray:
        """
        å°†pose encodingè½¬æ¢ä¸ºå¤–å‚çŸ©é˜µ
        
        Args:
            pose_enc: pose encoding (6,) æˆ– (9,) - æ”¯æŒ6Då’Œ9Dæ ¼å¼
                - 6Dæ ¼å¼: [tx, ty, tz, rx, ry, rz]
                - 9Dæ ¼å¼: [tx, ty, tz, qx, qy, qz, qw, fov_h, fov_w]
            
        Returns:
            å¤–å‚camera to worldçŸ©é˜µ (3, 4)
        """
        assert pose_enc.shape == (9,), f"pose_encå½¢çŠ¶é”™è¯¯: {pose_enc.shape}ï¼ŒæœŸæœ›(9,)"
        # 9Dæ ¼å¼: [tx, ty, tz, qx, qy, qz, qw, fov_h, fov_w]
        translation = pose_enc[:3]  # [tx, ty, tz]
        quaternion = pose_enc[3:7]  # [qx, qy, qz, qw]
        
        # å°†å››å…ƒæ•°è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µ
        import cv2
        rotation_matrix = cv2.Rodrigues(quaternion[:3])[0]  # ä½¿ç”¨å‰3ä¸ªåˆ†é‡ä½œä¸ºæ—‹è½¬å‘é‡
        
        # æ„å»ºå¤–å‚çŸ©é˜µ [R|t]
        extrinsics = np.eye(4)
        extrinsics[:3, :3] = rotation_matrix
        extrinsics[:3, 3] = translation
        
        return extrinsics[:3, :]  # è¿”å› (3, 4)
    
    def visualize_point_cloud_projection(
        self,
        points_3d: np.ndarray,
        point_colors: np.ndarray,
        camera_extrinsics: np.ndarray,
        camera_intrinsics: np.ndarray,
        image_shape: Tuple[int, int],
        need_inverse: bool = False
    ) -> np.ndarray:
        """
        å°†3Dç‚¹äº‘æŠ•å½±åˆ°æŒ‡å®šç›¸æœºè§†è§’å¹¶å¯è§†åŒ–ï¼ˆæŒ‰è·ç¦»æ’åºï¼Œè¿œçš„å…ˆç”»ï¼Œè¿‘çš„åç”»ï¼‰
        
        Args:
            points_3d: 3Dç‚¹äº‘åæ ‡ (N, 3) - ä¸–ç•Œåæ ‡ç³»
            point_colors: ç‚¹äº‘é¢œè‰² (N, 3)
            camera_extrinsics: ç›¸æœºå¤–å‚ (3, 4) - world2cameraå˜æ¢çŸ©é˜µ
            camera_intrinsics: ç›¸æœºå†…å‚ (3, 3) - GT intrinsic
            image_shape: è¾“å‡ºå›¾åƒå½¢çŠ¶ (H, W)
            need_inverse: æ˜¯å¦éœ€è¦å¯¹å¤–å‚æ±‚é€† (True for wrist, False for ext1/ext2)
            
        Returns:
            æŠ•å½±å›¾åƒ (H, W, 3)
        """
        import cv2

        H, W = image_shape

        # æ£€æŸ¥è¾“å…¥æ•°æ®
        assert points_3d.ndim == 2, f"ç‚¹äº‘ç»´åº¦é”™è¯¯: {points_3d.ndim}ï¼ŒæœŸæœ›2"
        assert points_3d.shape[1] == 3, f"ç‚¹äº‘å½¢çŠ¶é”™è¯¯: {points_3d.shape}ï¼ŒæœŸæœ›(N,3)"
        assert point_colors.ndim == 2, f"é¢œè‰²ç»´åº¦é”™è¯¯: {point_colors.ndim}ï¼ŒæœŸæœ›2"
        assert point_colors.shape[1] == 3, f"é¢œè‰²å½¢çŠ¶é”™è¯¯: {point_colors.shape}ï¼ŒæœŸæœ›(N,3)"
        assert len(points_3d) == len(point_colors), f"ç‚¹äº‘å’Œé¢œè‰²æ•°é‡ä¸åŒ¹é…: {len(points_3d)} vs {len(point_colors)}"
        assert camera_extrinsics.shape == (3, 4), f"ç›¸æœºå¤–å‚å½¢çŠ¶é”™è¯¯: {camera_extrinsics.shape}ï¼ŒæœŸæœ›(3,4)"
        assert camera_intrinsics.shape == (3, 3), f"ç›¸æœºå†…å‚å½¢çŠ¶é”™è¯¯: {camera_intrinsics.shape}ï¼ŒæœŸæœ›(3,3)"

        # æ ¹æ®need_inverseå‚æ•°å†³å®šæ˜¯å¦æ±‚é€†
        if need_inverse:
            # wristæŠ•å½±ï¼šcamera_extrinsicsæ˜¯camera2worldå˜æ¢çŸ©é˜µï¼Œéœ€è¦æ±‚é€†å¾—åˆ°world2camera
            camera_ext_4x4 = np.vstack([camera_extrinsics, [0, 0, 0, 1]])  # æ‰©å±•ä¸º4x4é½æ¬¡åæ ‡çŸ©é˜µ
            world2camera_ext = np.linalg.inv(camera_ext_4x4)[:3, :4]  # æ±‚é€†å¾—åˆ°world2cameraå˜æ¢
        else:
            # ext1/ext2æŠ•å½±ï¼šcamera_extrinsicså·²ç»æ˜¯world2cameraå˜æ¢çŸ©é˜µï¼Œç›´æ¥ä½¿ç”¨
            world2camera_ext = camera_extrinsics

        # åˆ›å»ºè¾“å‡ºå›¾åƒ
        image = np.full((H, W, 3), (0, 0, 0), dtype=np.uint8)

        # å‘é‡åŒ–å¤„ç†æ‰€æœ‰3Dç‚¹
        # è½¬æ¢ä¸ºé½æ¬¡åæ ‡ [N, 4]
        points_homo = np.concatenate([points_3d, np.ones((len(points_3d), 1))], axis=1)

        # æŠ•å½±åˆ°ç›¸æœºåæ ‡ç³» [N, 3]
        points_cam = (world2camera_ext @ points_homo.T).T

        # æ·±åº¦è¿‡æ»¤mask
        depth_mask = points_cam[:, 2] > 0.01
        if not np.any(depth_mask):
            return image

        # åº”ç”¨æ·±åº¦è¿‡æ»¤
        points_cam = points_cam[depth_mask]
        point_colors = point_colors[depth_mask]

        # æŠ•å½±åˆ°å›¾åƒå¹³é¢ [N, 2]
        points_2d = points_cam[:, :2] / points_cam[:, 2:3]

        # åº”ç”¨å†…å‚ [N, 2]
        points_2d_homo = np.concatenate([points_2d, np.ones((len(points_2d), 1))], axis=1)
        points_pixel = (camera_intrinsics @ points_2d_homo.T).T
        projected_uv = points_pixel[:, :2]

        # è¾¹ç•Œæ£€æŸ¥mask
        u_mask = (projected_uv[:, 0] >= 0) & (projected_uv[:, 0] < W)
        v_mask = (projected_uv[:, 1] >= 0) & (projected_uv[:, 1] < H)
        boundary_mask = u_mask & v_mask

        if not np.any(boundary_mask):
            return image

        # åº”ç”¨è¾¹ç•Œè¿‡æ»¤
        projected_uv = projected_uv[boundary_mask]
        point_colors = point_colors[boundary_mask]
        points_cam = points_cam[boundary_mask]

        # æŒ‰è·ç¦»æ’åºï¼ˆzè¶Šå¤§è¶Šè¿œï¼Œå…ˆç”»è¿œçš„ï¼‰
        z_vals = points_cam[:, 2]
        sort_idx = np.argsort(z_vals)[::-1]  # ä»è¿œåˆ°è¿‘ï¼ˆzå¤§åˆ°zå°ï¼‰
        projected_uv = projected_uv[sort_idx]
        point_colors = point_colors[sort_idx]

        # è½¬æ¢ä¸ºæ•´æ•°åæ ‡
        u_coords = projected_uv[:, 0].astype(int)
        v_coords = projected_uv[:, 1].astype(int)

        # å¤„ç†é¢œè‰²æ ¼å¼ï¼ˆå‘é‡åŒ–ï¼‰
        if point_colors.max() < 2:
            point_colors = (point_colors * 255).astype(np.uint8)

        # æŒ‰é¡ºåºç”»ç‚¹ï¼ˆè¿œçš„å…ˆç”»ï¼Œè¿‘çš„åç”»ï¼‰
        for i in range(len(u_coords)):
            u, v = u_coords[i], v_coords[i]
            color = point_colors[i].tolist()
            cv2.circle(image, (u, v), 2, color, -1)

        valid_count = len(u_coords)

        return image
    
    def _add_wrist_origin_sphere(self, predictions: Dict, points_3d: np.ndarray, colors: np.ndarray) -> Dict:
        """
        åœ¨ç‚¹äº‘ä¸­æ·»åŠ çº¢çƒè¡¨ç¤ºé¢„æµ‹çš„wrist origin
        
        Args:
            predictions: åŒ…å«wrist poseé¢„æµ‹çš„predictionsæ•°æ®
            points_3d: åŸå§‹ç‚¹äº‘åæ ‡ (N, 3)
            colors: åŸå§‹ç‚¹äº‘é¢œè‰² (N, 3)
            
        Returns:
            åŒ…å«çº¢çƒçš„ç‚¹äº‘æ•°æ® {"points": ..., "colors": ...}
        """
        # è·å–é¢„æµ‹çš„wrist pose
        wrist_pose_enc = predictions.get("wrist_pose_enc")
        if wrist_pose_enc is None:
            return {"points": points_3d, "colors": colors}
        
        # å¤„ç†batchç»´åº¦å’Œpose encodingæ ¼å¼
        if torch.is_tensor(wrist_pose_enc):
            wrist_pose_enc = wrist_pose_enc.cpu().numpy()
        
        # ğŸ”¥ NEW: wrist_head now outputs single wrist pose [B, 1, target_dim] instead of [B, S, target_dim]
        wrist_pose_enc = wrist_pose_enc[0]  # å–ç¬¬0ä¸ªæ ·æœ¬
        assert wrist_pose_enc.shape == (1, 9), f"wrist_pose_encå½¢çŠ¶é”™è¯¯: {wrist_pose_enc.shape}ï¼ŒæœŸæœ›(1,9)"
        # ç°åœ¨åªæœ‰ä¸€ä¸ªwrist poseï¼Œ9Dæ ¼å¼
        wrist_pose = wrist_pose_enc[0]  # å–å”¯ä¸€çš„wrist poseï¼Œ9Dæ ¼å¼
        
        # è½¬æ¢ä¸ºcamera-to-worldå¤–å‚çŸ©é˜µ
        wrist_ext = self._pose_to_extrinsics(wrist_pose)
        
        # æå–wrist originä½ç½®
        # wrist_extæ˜¯camera2worldå˜æ¢çŸ©é˜µT_wc
        wrist_origin = wrist_ext[:3, 3]  # å–é€†çŸ©é˜µçš„å¹³ç§»éƒ¨åˆ†
        
        # ç”Ÿæˆçº¢çƒç‚¹äº‘
        sphere_points, sphere_colors = self._generate_sphere_points(
            center=wrist_origin,
            radius=0.05,  # 5cmåŠå¾„
            color=(255, 0, 0),  # çº¢è‰²
            num_points=100
        )
        
        
        # åˆå¹¶åŸå§‹ç‚¹äº‘å’Œçº¢çƒ
        combined_points = np.vstack([points_3d, sphere_points])
        combined_colors = np.vstack([colors, sphere_colors])
        
        return {"points": combined_points, "colors": combined_colors}
    
    def _add_gt_wrist_origin_sphere(self, batch: Dict, points_3d: np.ndarray, colors: np.ndarray) -> Dict:
        """
        åœ¨ç‚¹äº‘ä¸­æ·»åŠ ç»¿çƒè¡¨ç¤ºGT wrist origin
        
        Args:
            batch: åŒ…å«GT wrist poseçš„batchæ•°æ®
            points_3d: åŸå§‹ç‚¹äº‘åæ ‡ (N, 3)
            colors: åŸå§‹ç‚¹äº‘é¢œè‰² (N, 3)
            
        Returns:
            åŒ…å«ç»¿çƒçš„ç‚¹äº‘æ•°æ® {"points": ..., "colors": ...}
        """
        # è·å–GT wrist pose
        wrist_extrinsics = batch.get("wrist_extrinsics")
        if wrist_extrinsics is None:
            return {"points": points_3d, "colors": colors}
        
        wrist_ext = wrist_extrinsics[0][0].cpu().numpy()
        
        assert wrist_ext.shape == (3, 4), f"wristå¤–å‚å½¢çŠ¶é”™è¯¯: {wrist_ext.shape}ï¼ŒæœŸæœ›(3,4)"
        
        # æå–GT wrist originä½ç½®
        # wrist_extæ˜¯world2cameraå˜æ¢çŸ©é˜µT_wc
        # è¦å¾—åˆ°ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„ä½ç½®ï¼Œéœ€è¦æ±‚é€†ï¼šT_cw = inv(T_wc)
        # ç›¸æœºä½ç½® = T_cw * [0,0,0,1] = T_cwçš„å¹³ç§»éƒ¨åˆ†
        wrist_ext_4x4 = np.vstack([wrist_ext, [0, 0, 0, 1]])  # æ‰©å±•ä¸º4x4é½æ¬¡åæ ‡çŸ©é˜µ
        wrist_ext_inv = np.linalg.inv(wrist_ext_4x4)  # æ±‚é€†å¾—åˆ°camera2worldå˜æ¢
        gt_wrist_origin = wrist_ext_inv[:3, 3]  # å–é€†çŸ©é˜µçš„å¹³ç§»éƒ¨åˆ†
        
        # ç”Ÿæˆç»¿çƒç‚¹äº‘
        sphere_points, sphere_colors = self._generate_sphere_points(
            center=gt_wrist_origin,
            radius=0.05,  # 5cmåŠå¾„
            color=(0, 255, 0),  # ç»¿è‰²
            num_points=100
        )
        
        
        # åˆå¹¶åŸå§‹ç‚¹äº‘å’Œç»¿çƒ
        combined_points = np.vstack([points_3d, sphere_points])
        combined_colors = np.vstack([colors, sphere_colors])
        
        return {"points": combined_points, "colors": combined_colors}
    
    def _generate_sphere_points(self, center: np.ndarray, radius: float, color: Tuple[int, int, int], num_points: int = 100) -> Tuple[np.ndarray, np.ndarray]:
        """
        ç”Ÿæˆçƒä½“ç‚¹äº‘
        
        Args:
            center: çƒå¿ƒåæ ‡ (3,)
            radius: çƒåŠå¾„
            color: çƒé¢œè‰² (R, G, B)
            num_points: çƒä½“ç‚¹æ•°
            
        Returns:
            çƒä½“ç‚¹äº‘åæ ‡å’Œé¢œè‰²
        """
        # ç”Ÿæˆçƒé¢å‡åŒ€åˆ†å¸ƒçš„ç‚¹
        phi = np.linspace(0, 2*np.pi, int(np.sqrt(num_points)))
        theta = np.linspace(0, np.pi, int(np.sqrt(num_points)))
        phi, theta = np.meshgrid(phi, theta)
        
        # çƒåæ ‡è½¬ç¬›å¡å°”åæ ‡
        x = radius * np.sin(theta) * np.cos(phi)
        y = radius * np.sin(theta) * np.sin(phi)
        z = radius * np.cos(theta)
        
        # å±•å¹³å¹¶æ·»åŠ çƒå¿ƒåç§»
        sphere_points = np.concatenate([x.flatten(), y.flatten(), z.flatten()]).reshape(-1, 3)
        sphere_points = sphere_points + center  # ä½¿ç”¨numpyå¹¿æ’­æœºåˆ¶
        
        # ç”Ÿæˆé¢œè‰²
        sphere_colors = np.full((len(sphere_points), 3), color, dtype=np.uint8)
        
        return sphere_points, sphere_colors
    
    def _save_point_cloud_as_glb(self, points: np.ndarray, colors: np.ndarray, output_path: str):
        """
        å°†ç‚¹äº‘ä¿å­˜ä¸ºGLBæ ¼å¼
        
        Args:
            points: ç‚¹äº‘åæ ‡ (N, 3)
            colors: ç‚¹äº‘é¢œè‰² (N, 3)
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # åˆ›å»ºtrimeshç‚¹äº‘å¯¹è±¡
        point_cloud = trimesh.PointCloud(
            vertices=points,
            colors=colors
        )
        
        # å¯¼å‡ºä¸ºGLBæ ¼å¼
        point_cloud.export(output_path)
    
    def _visualize_projection_tracking(
        self,
        predictions: Dict,
        batch: Dict,
        output_dir: Path,
    ) -> Dict[str, str]:
        """
        å¯è§†åŒ–projection trackingç»“æœ
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœ
            batch: è¾“å…¥batchæ•°æ®
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        import cv2
        import numpy as np
        from vggt.utils.pose_enc import pose_encoding_to_extri_intri
        is_single_view_data = False
        if "single_view_training" in batch:
            is_single_view_data = batch["single_view_training"][0].item() if torch.is_tensor(batch["single_view_training"]) else batch["single_view_training"][0]
        
        # åªå¤„ç†batch_index=0çš„æ•°æ®
        track_pairs = batch["track_pairs"]
        if len(track_pairs.get("wrist_uv", [])) == 0:
            print("no track pairs")
            return {}
        
        # è¿‡æ»¤å‡ºbatch_index=0çš„track pairs
        if "batch_indices" in track_pairs:
            batch_indices = track_pairs["batch_indices"]
            batch_0_mask = [i == 0 for i in batch_indices]
            
            # æå–batch_0çš„æ•°æ®
            wrist_uv_batch0 = [track_pairs["wrist_uv"][i] for i, mask in enumerate(batch_0_mask) if mask]
            
            # ğŸ¯ æ–°çš„æ•°æ®ç»“æ„ï¼šä½¿ç”¨ç»Ÿä¸€çš„ext_uvå­—æ®µ
            if "ext_uv" in track_pairs:
                ext_uv_batch0 = [track_pairs["ext_uv"][i] for i, mask in enumerate(batch_0_mask) if mask]
            else:
                # æ—§çš„å…¼å®¹æ€§æ•°æ®ç»“æ„
                ext1_uv_batch0 = [track_pairs["ext1_uv"][i] for i, mask in enumerate(batch_0_mask) if mask]
                ext2_uv_batch0 = [track_pairs["ext2_uv"][i] for i, mask in enumerate(batch_0_mask) if mask]
            pair_type_batch0 = [track_pairs["pair_type"][i] for i, mask in enumerate(batch_0_mask) if mask]
            
            confidence_batch0 = [track_pairs["confidence"][i] for i, mask in enumerate(batch_0_mask) if mask]
        # print(len(wrist_uv_batch0),len(track_pairs["wrist_uv"]))
        if len(wrist_uv_batch0) == 0:
            return {}
        
        # è·å–wrist RGBå›¾åƒ
        wrist_rgb = batch["wrist_image"][0].cpu().numpy()  # å–batch_0
        wrist_rgb = wrist_rgb.astype(np.uint8)
        
        # Resize wrist_rgbä»1280x720åˆ°518x294
        # print(wrist_rgb.shape)
        # if wrist_rgb.shape[1] == 1280:
        wrist_rgb = cv2.resize(wrist_rgb, (518, 294))
        # else:
        #     wrist_rgb = cv2.resize(wrist_rgb, (518, 518))
        
        H, W = wrist_rgb.shape[:2]
        
        # è·å–é¢„æµ‹çš„wrist poseå’ŒGT wrist intrinsics
        wrist_pose_enc = predictions["wrist_pose_enc_list"][-1][0]  # [1, 9] - batch_0
        wrist_pose_enc = wrist_pose_enc[0]  # [9] - å–å”¯ä¸€çš„wrist pose
        
        # è½¬æ¢ä¸ºextrinsicå’Œintrinsic
        wrist_ext_pred, _ = pose_encoding_to_extri_intri(
            wrist_pose_enc.unsqueeze(0).unsqueeze(0),  # [1, 1, 9]
            image_size_hw=(H, W),
            build_intrinsics=False
        )
        wrist_ext_pred = wrist_ext_pred[0, 0].cpu().numpy()  # [3, 4]
        
        # ä½¿ç”¨GT wrist intrinsics
        wrist_intrinsics_gt = batch["wrist_intrinsics"][0].cpu().numpy()  # [3, 3] - batch_0
        
        # è·å–é¢„æµ‹çš„world points
        world_points = predictions["world_points"][0]  # [2, H, W, 3] - batch_0
        
        # åˆ›å»ºå¯è§†åŒ–å›¾åƒ
        # 1. çœŸå®wrist view + trackç‚¹
        wrist_with_tracks = wrist_rgb.copy()
        
        # 2. ç”Ÿæˆwrist point cloud projection
        wrist_projection = self._generate_wrist_point_cloud_projection(
            predictions=predictions,
            batch=batch,
            image_shape=(H, W)
        )
        
        # 3. å¯¹æ¯”å›¾åƒï¼ˆå·¦å³æ‹¼æ¥ï¼‰
        comparison_img = np.zeros((H, W*2, 3), dtype=np.uint8)
        # å¤„ç†æ¯ä¸ªtrack pair
        valid_projections = 0
        total_pairs = len(wrist_uv_batch0)
        comparison_img[:, :W] = wrist_with_tracks  # å·¦åŠè¾¹ï¼šçœŸå®wrist view + trackç‚¹
        comparison_img[:, W:] = wrist_projection   # å³åŠè¾¹ï¼šwrist point cloud projection
        
        for i in range(total_pairs):
            wrist_uv = wrist_uv_batch0[i]
            pair_type = pair_type_batch0[i]
            confidence = confidence_batch0[i]
            
            # è·³è¿‡ä½ç½®ä¿¡åº¦çš„ç‚¹
            if confidence < 0.1:
                continue
            
            # åœ¨çœŸå®wrist viewä¸Šç”»ç‚¹
            wrist_u, wrist_v = int(wrist_uv[0]), int(wrist_uv[1])
            if 0 <= wrist_u < W and 0 <= wrist_v < H:
                cv2.circle(comparison_img, (wrist_u, wrist_v), 3, (0, 255, 0), -1)  # ç»¿è‰²ç‚¹
            ext_uv = ext_uv_batch0[i]
            world_points_seq = pair_type
            
            # è·³è¿‡æ— æ•ˆçš„ext UVåæ ‡
            if ext_uv[0] < 0 or ext_uv[1] < 0:
                continue
            
            # ä»world pointsè·å–3Dç‚¹
            try:
                point_3d = self._get_interpolated_3d_point_numpy(
                    world_points[world_points_seq].cpu().numpy(),  # [H, W, 3]
                    ext_uv[0],  # u coordinate
                    ext_uv[1]   # v coordinate
                )
                
                # æŠ•å½±åˆ°wrist view
                projected_uv, depth, is_valid = self._project_3d_to_wrist_numpy(
                    point_3d,
                    wrist_ext_pred,
                    wrist_intrinsics_gt
                )
                if is_valid:
                    
                    # print(is_valid,projected_uv)
                    # åœ¨æŠ•å½±å›¾åƒä¸Šç”»ç‚¹
                    proj_u, proj_v = int(projected_uv[0][0]), int(projected_uv[0][1])
                    if 0 <= proj_u < W and 0 <= proj_v < H:
                        cv2.circle(comparison_img, (W + proj_u, proj_v), 3, (255, 0, 0), -1)  # çº¢è‰²ç‚¹
                        cv2.circle(comparison_img, (wrist_u, wrist_v), 3, (0, 255, 0), -1)  # ç»¿è‰²ç‚¹
                        cv2.line(comparison_img, (wrist_u, wrist_v), (W + proj_u, proj_v), (255, 255, 255), 1)
                        
                        valid_projections += 1
                
            except Exception as e:
                print(e)
                import traceback
                traceback.print_exc()
                continue
        
        # ä¿å­˜å›¾åƒ
        comparison_path = output_dir / "wrist_tracking_comparison.png"
        
        cv2.imwrite(str(comparison_path), cv2.cvtColor(comparison_img, cv2.COLOR_RGB2BGR))
        
        return {
            "wrist_tracking_comparison": str(comparison_path),
            "valid_projections": valid_projections,
            "total_track_pairs": total_pairs
        }
    
    def _get_interpolated_3d_point_numpy(
        self,
        world_points_map: np.ndarray,
        u: float,
        v: float
    ) -> np.ndarray:
        """
        ä½¿ç”¨numpyè¿›è¡ŒåŒçº¿æ€§æ’å€¼è·å–3Dç‚¹
        
        Args:
            world_points_map: 3Dä¸–ç•Œç‚¹äº‘å›¾ [H, W, 3]
            u: Uåæ ‡ï¼ˆæµ®ç‚¹æ•°ï¼‰
            v: Våæ ‡ï¼ˆæµ®ç‚¹æ•°ï¼‰
            
        Returns:
            3Dç‚¹ [3]
        """
        H, W, _ = world_points_map.shape
        
        # è·å–å››ä¸ªè§’ç‚¹ç´¢å¼•
        u0, u1 = int(np.floor(u)), int(np.ceil(u))
        v0, v1 = int(np.floor(v)), int(np.ceil(v))
        
        # è®¡ç®—æ’å€¼æƒé‡
        wu = u - u0
        wv = v - v0
        
        # å¤„ç†è¾¹ç•Œæƒ…å†µ
        if u0 == u1:
            wu = 0.0
        if v0 == v1:
            wv = 0.0
        
        # è·å–å››ä¸ªè§’ç‚¹ï¼ˆå¤„ç†è¾¹ç•Œï¼‰
        u0_clamped = max(0, min(u0, W-1))
        u1_clamped = max(0, min(u1, W-1))
        v0_clamped = max(0, min(v0, H-1))
        v1_clamped = max(0, min(v1, H-1))
        
        p00 = world_points_map[v0_clamped, u0_clamped, :]
        p01 = world_points_map[v0_clamped, u1_clamped, :]
        p10 = world_points_map[v1_clamped, u0_clamped, :]
        p11 = world_points_map[v1_clamped, u1_clamped, :]
        
        # åŒçº¿æ€§æ’å€¼
        point_3d = (1-wu)*(1-wv)*p00 + wu*(1-wv)*p01 + (1-wu)*wv*p10 + wu*wv*p11
        
        return point_3d
    
    def _project_3d_to_wrist_numpy(
        self,
        point_3d: np.ndarray,
        wrist_ext: np.ndarray,
        wrist_intrinsics: np.ndarray
    ) -> Tuple[np.ndarray, float, bool]:
        """
        ä½¿ç”¨numpyå°†3Dç‚¹æŠ•å½±åˆ°wristç›¸æœºè§†è§’
        
        Args:
            point_3d: ä¸–ç•Œåæ ‡ç³»ä¸­çš„3Dç‚¹ [3]
            wrist_ext: wristç›¸æœºå¤–å‚çŸ©é˜µ [3, 4] - world2cameraå˜æ¢çŸ©é˜µ
            wrist_intrinsics: wristç›¸æœºå†…å‚çŸ©é˜µ [3, 3]
            
        Returns:
            Tuple of (projected_uv, depth, is_valid)
            - projected_uv: [2] - æŠ•å½±çš„UVåæ ‡
            - depth: æ·±åº¦å€¼
            - is_valid: æ˜¯å¦æœ‰æ•ˆæŠ•å½±
        """
        # å¯¹å¤–å‚æ±‚é€†ï¼šä»camera2worldå˜ä¸ºworld2camera
        wrist_ext_4x4 = np.vstack([wrist_ext, [0, 0, 0, 1]])  # æ‰©å±•ä¸º4x4é½æ¬¡åæ ‡çŸ©é˜µ
        world2camera_ext = wrist_ext_4x4[:3, :4]  # æ±‚é€†å¾—åˆ°world2cameraå˜æ¢
        
        # è½¬æ¢ä¸ºé½æ¬¡åæ ‡
        point_homo = np.append(point_3d, 1.0)  # [4]
        
        # æŠ•å½±åˆ°ç›¸æœºåæ ‡ç³»
        point_cam = world2camera_ext @ point_homo  # [3]
        
        # æ£€æŸ¥æ·±åº¦æ˜¯å¦ä¸ºæ­£
        depth = point_cam[2]
        if depth <= 0.01:
            return np.array([0, 0]), depth, False
        
        # æŠ•å½±åˆ°å›¾åƒå¹³é¢
        point_2d = point_cam[:2] / depth  # [2]
        
        # åº”ç”¨å†…å‚
        point_2d_homo = np.append(point_2d, 1.0)  # [3]
        point_pixel = wrist_intrinsics @ point_2d_homo  # [3]
        projected_uv = point_pixel[:2]  # [2]
        
        return projected_uv, depth, True
    
    def _generate_wrist_point_cloud_projection(
        self,
        predictions: Dict,
        batch: Dict,
        image_shape: Tuple[int, int]
    ) -> np.ndarray:
        """
        ç”Ÿæˆwrist point cloud projectionå›¾åƒ
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœ
            batch: è¾“å…¥batchæ•°æ®
            image_shape: è¾“å‡ºå›¾åƒå½¢çŠ¶ (H, W)
            
        Returns:
            wrist point cloud projectionå›¾åƒ (H, W, 3)
        """
        H, W = image_shape
        
        # è·å–é¢„æµ‹çš„wrist poseå’Œworld points
        wrist_pose_enc = predictions["wrist_pose_enc_list"][-1][0]  # [1, 9] - batch_0
        wrist_pose_enc = wrist_pose_enc[0]  # [9] - å–å”¯ä¸€çš„wrist pose
        
        # è½¬æ¢ä¸ºextrinsicå’Œintrinsic
        wrist_ext_pred, _ = pose_encoding_to_extri_intri(
            wrist_pose_enc.unsqueeze(0).unsqueeze(0),  # [1, 1, 9]
            image_size_hw=(H, W),
            build_intrinsics=False
        )
        wrist_ext_pred = wrist_ext_pred[0, 0].cpu().numpy()  # [3, 4]
        
        # ä½¿ç”¨GT wrist intrinsics
        wrist_intrinsics_gt = batch["wrist_intrinsics"][0].cpu().numpy()  # [3, 3] - batch_0
        
        # è·å–é¢„æµ‹çš„world points
        world_points = predictions["world_points"][0]  # [2, H, W, 3] - batch_0
        
        # åˆå¹¶æ‰€æœ‰è§†è§’çš„world points
        all_world_points = world_points.reshape(-1, 3)  # [N, 3]
        
        # ç”Ÿæˆé¢œè‰²ï¼ˆä½¿ç”¨æ‰€æœ‰è§†è§’çš„RGBï¼‰
        images = batch["images"][0]  # [S, C, H, W] - batch_0
        colors_list = []
        for cam_idx in range(images.shape[0]):
            rgb = images[cam_idx]
            if torch.is_tensor(rgb):
                rgb = rgb.cpu().numpy()
            rgb = np.transpose(rgb, (1, 2, 0))
            assert rgb.shape[-1] == 3, f"RGBå½¢çŠ¶é”™è¯¯: {rgb.shape}"
            colors_list.append(rgb.reshape(-1, 3))
        colors = np.concatenate(colors_list, axis=0)
        
        # ç¡®ä¿ç‚¹äº‘å’Œé¢œè‰²æ•°é‡åŒ¹é…
        if len(all_world_points) != len(colors):
            logging.warning(f"Wrist projection: ç‚¹äº‘å’Œé¢œè‰²æ•°é‡ä¸åŒ¹é…: {len(all_world_points)} vs {len(colors)}")
            # å¦‚æœæ•°é‡ä¸åŒ¹é…ï¼Œæˆªå–åˆ°è¾ƒå°çš„æ•°é‡
            min_count = min(len(all_world_points), len(colors))
            all_world_points = all_world_points[:min_count]
            colors = colors[:min_count]
        
        # æŠ•å½±åˆ°wristè§†è§’
        wrist_projection = self.visualize_point_cloud_projection(
            points_3d=all_world_points.cpu().numpy(),
            point_colors=colors,
            camera_extrinsics=wrist_ext_pred,
            camera_intrinsics=wrist_intrinsics_gt[0],  # ä½¿ç”¨GT intrinsic
            image_shape=(H, W),
            need_inverse=False  # å¯¹äºwristï¼Œæ˜¯world2cameraï¼Œä¸éœ€è¦æ±‚é€†
        )
        
        return wrist_projection