# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import os
import os.path as osp
import logging
import random
import numpy as np
import cv2
from PIL import Image
from typing import Dict, List, Tuple, Optional

from data.dataset_util import *
from data.base_dataset import BaseDataset


class VggtVideoDataset(BaseDataset):
    """
    VGGT Video Dataset for training with video input.
    
    This dataset implements:
    1. Loads frames from three video files: ext1, ext2, wrist
    2. Uses all frames from the videos as both train and val sets
    3. Generates zeros for GT data (depth, point clouds, etc.)
    4. Supports wrist pose prediction
    """
    
    def __init__(
        self,
        common_config,
        split: str = "train",
        ext1_video_path: str = "",
        ext2_video_path: str = "",
        wrist_video_path: str = "",
        enable_wrist_prediction: bool = True,
    ):
        """
        Initialize the VggtVideoDataset.

        Args:
            common_config: Configuration object with common settings.
            split (str): Dataset split, either 'train' or 'test' (both use same data).
            ext1_video_path (str): Path to ext1 video file.
            ext2_video_path (str): Path to ext2 video file.
            wrist_video_path (str): Path to wrist video file.
            enable_wrist_prediction (bool): Whether to enable wrist pose prediction.
        """
        super().__init__(common_conf=common_config)

        self.debug = common_config.debug
        self.training = common_config.training
        self.get_nearby = common_config.get_nearby
        self.load_depth = common_config.load_depth
        self.inside_random = common_config.inside_random
        self.allow_duplicate_img = common_config.allow_duplicate_img
        self.enable_wrist_prediction = enable_wrist_prediction

        # éªŒè¯è§†é¢‘æ–‡ä»¶è·¯å¾„
        if not ext1_video_path or not ext2_video_path or not wrist_video_path:
            raise ValueError("æ‰€æœ‰ä¸‰ä¸ªè§†é¢‘è·¯å¾„éƒ½å¿…é¡»æä¾›: ext1_video_path, ext2_video_path, wrist_video_path")
        
        if not osp.exists(ext1_video_path):
            raise FileNotFoundError(f"ext1è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {ext1_video_path}")
        if not osp.exists(ext2_video_path):
            raise FileNotFoundError(f"ext2è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {ext2_video_path}")
        if not osp.exists(wrist_video_path):
            raise FileNotFoundError(f"wristè§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {wrist_video_path}")

        self.ext1_video_path = ext1_video_path
        self.ext2_video_path = ext2_video_path
        self.wrist_video_path = wrist_video_path

        # åŠ è½½è§†é¢‘å¸§
        self.ext1_frames = self._load_video_frames(ext1_video_path)
        self.ext2_frames = self._load_video_frames(ext2_video_path)
        self.wrist_frames = self._load_video_frames(wrist_video_path)

        # æ£€æŸ¥å¸§æ•°ä¸€è‡´æ€§
        frame_counts = [len(self.ext1_frames), len(self.ext2_frames), len(self.wrist_frames)]
        if not all(count == frame_counts[0] for count in frame_counts):
            logging.warning(f"è§†é¢‘å¸§æ•°ä¸ä¸€è‡´: ext1={frame_counts[0]}, ext2={frame_counts[1]}, wrist={frame_counts[2]}")
            # ä½¿ç”¨æœ€å°å¸§æ•°
            min_frames = min(frame_counts)
            self.ext1_frames = self.ext1_frames[:min_frames]
            self.ext2_frames = self.ext2_frames[:min_frames]
            self.wrist_frames = self.wrist_frames[:min_frames]

        self.total_frames = len(self.ext1_frames)
        
        # è·å–è§†é¢‘ä¿¡æ¯
        self.video_info = self._get_video_info(ext1_video_path)
        
        status = "Training" if self.training else "Test"
        logging.info(f"{status}: VGGT-Video Dataset - æ€»å¸§æ•°: {self.total_frames}")
        logging.info(f"{status}: è§†é¢‘åˆ†è¾¨ç‡: {self.video_info['width']}x{self.video_info['height']}")
        logging.info(f"{status}: è§†é¢‘FPS: {self.video_info['fps']}")

    def _load_video_frames(self, video_path: str) -> List[np.ndarray]:
        """
        åŠ è½½è§†é¢‘å¸§
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            å¸§åˆ—è¡¨ (RGBæ ¼å¼)
        """
        logging.info(f"æ­£åœ¨åŠ è½½è§†é¢‘å¸§: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        frames = []
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        for i in range(frame_count):
            ret, frame = cap.read()
            if not ret:
                break
            
            # è½¬æ¢BGRåˆ°RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame_rgb)
        
        cap.release()
        logging.info(f"âœ… åŠ è½½å®Œæˆï¼Œå¸§æ•°: {len(frames)}")
        return frames

    def _get_video_info(self, video_path: str) -> Dict:
        """
        è·å–è§†é¢‘ä¿¡æ¯
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            è§†é¢‘ä¿¡æ¯å­—å…¸
        """
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        info = {
            'fps': cap.get(cv2.CAP_PROP_FPS),
            'frame_count': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),
            'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
            'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        }
        
        cap.release()
        return info

    def _generate_zeros_depth(self, height: int, width: int) -> np.ndarray:
        """
        ç”Ÿæˆzerosæ·±åº¦å›¾
        
        Args:
            height: å›¾åƒé«˜åº¦
            width: å›¾åƒå®½åº¦
            
        Returns:
            zerosæ·±åº¦å›¾
        """
        return np.zeros((height, width, 1), dtype=np.float32)

    def _generate_zeros_point_cloud(self, height: int, width: int) -> np.ndarray:
        """
        ç”Ÿæˆzerosç‚¹äº‘
        
        Args:
            height: å›¾åƒé«˜åº¦
            width: å›¾åƒå®½åº¦
            
        Returns:
            zerosç‚¹äº‘ (H*W, 3)
        """
        return np.zeros((height * width, 3), dtype=np.float32)

    def _generate_zeros_camera_params(self) -> Dict:
        """
        ç”Ÿæˆåˆç†çš„ç›¸æœºå‚æ•°
        
        Returns:
            ç›¸æœºå‚æ•°å­—å…¸
        """
        # ä½¿ç”¨åˆç†çš„ç›¸æœºå†…å‚ï¼ˆåŸºäºDROIDæ•°æ®é›†ï¼‰
        # å‡è®¾å›¾åƒå°ºå¯¸ä¸º294x518ï¼Œç„¦è·çº¦ä¸ºå›¾åƒå®½åº¦çš„ä¸€åŠ
        fx = fy = 259.0  # ç„¦è·çº¦ä¸ºå›¾åƒå®½åº¦çš„ä¸€åŠ
        cx = 259.0  # ä¸»ç‚¹xåæ ‡ï¼ˆå›¾åƒå®½åº¦çš„ä¸€åŠï¼‰
        cy = 147.0  # ä¸»ç‚¹yåæ ‡ï¼ˆå›¾åƒé«˜åº¦çš„ä¸€åŠï¼‰
        
        intrinsic = np.array([
            [fx, 0.0, cx],
            [0.0, fy, cy],
            [0.0, 0.0, 1.0]
        ], dtype=np.float32)
        
        # å¤–å‚ä½¿ç”¨å•ä½çŸ©é˜µï¼ˆå‡è®¾ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»åŸç‚¹ï¼‰
        extrinsic = np.eye(4, dtype=np.float32)
        
        return {
            'extrinsic': extrinsic,
            'intrinsic': intrinsic
        }

    def _preprocess_frame(self, frame: np.ndarray, target_size: Tuple[int, int] = (294, 518)) -> np.ndarray:
        """
        é¢„å¤„ç†å¸§
        
        Args:
            frame: è¾“å…¥å¸§ (H, W, 3)
            target_size: ç›®æ ‡å°ºå¯¸ (H, W) - å›ºå®šä¸º(294, 518)ä»¥åŒ¹é…DROIDæ•°æ®é›†æ¯”ä¾‹
            
        Returns:
            é¢„å¤„ç†åçš„å¸§
        """
        # è°ƒæ•´å°ºå¯¸
        frame_pil = Image.fromarray(frame)
        frame_resized = frame_pil.resize(target_size, Image.Resampling.BICUBIC)
        frame_array = np.array(frame_resized)
        
        # å½’ä¸€åŒ–åˆ°[0, 1]
        frame_normalized = frame_array.astype(np.float32) / 255.0
        
        return frame_normalized

    def get_data(
        self,
        seq_index: int = None,
        img_per_seq: int = 2,  # å›ºå®šä¸º2 (ext1 + ext2)
        seq_name: str = None,
        ids: list = None,
        aspect_ratio: float = 1.0,
    ) -> dict:
        """
        è·å–æ•°æ®
        
        Args:
            seq_index: åºåˆ—ç´¢å¼•ï¼ˆè¿™é‡Œç”¨ä½œå¸§ç´¢å¼•ï¼‰
            img_per_seq: æ¯ä¸ªåºåˆ—çš„å›¾åƒæ•°é‡ï¼ˆå›ºå®šä¸º2ï¼‰
            seq_name: åºåˆ—åç§°ï¼ˆæœªä½¿ç”¨ï¼‰
            ids: IDåˆ—è¡¨ï¼ˆæœªä½¿ç”¨ï¼‰
            aspect_ratio: å®½é«˜æ¯”ï¼ˆæœªä½¿ç”¨ï¼‰
            
        Returns:
            æ•°æ®å­—å…¸
        """
        if seq_index is None:
            # éšæœºé€‰æ‹©ä¸€å¸§
            seq_index = random.randint(0, self.total_frames - 1)
        
        # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
        seq_index = seq_index % self.total_frames
        
        # è·å–å¯¹åº”å¸§
        ext1_frame = self.ext1_frames[seq_index]
        ext2_frame = self.ext2_frames[seq_index]
        wrist_frame = self.wrist_frames[seq_index]
        
        # é¢„å¤„ç†å¸§
        target_size = (294, 518)  # å›ºå®šå°ºå¯¸ï¼ŒåŒ¹é…DROIDæ•°æ®é›†æ¯”ä¾‹
        ext1_processed = self._preprocess_frame(ext1_frame, target_size)
        ext2_processed = self._preprocess_frame(ext2_frame, target_size)
        wrist_processed = self._preprocess_frame(wrist_frame, target_size)
        
        # ç”ŸæˆGTæ•°æ®ï¼ˆzerosï¼‰
        height, width = target_size
        zeros_depth = self._generate_zeros_depth(height, width)
        zeros_point_cloud = self._generate_zeros_point_cloud(height, width)
        zeros_camera_params = self._generate_zeros_camera_params()
        
        # æ„å»ºæ•°æ®å­—å…¸
        data = {
            'images': np.stack([ext1_processed, ext2_processed], axis=0),  # (2, H, W, 3)
            'wrist_image': wrist_processed[np.newaxis, ...],  # (1, H, W, 3)
            'depth': zeros_depth[np.newaxis, ...],  # (1, H, W, 1)
            'world_points': zeros_point_cloud[np.newaxis, ...],  # (1, H*W, 3)
            # ğŸ”¥ æ·»åŠ è®­ç»ƒå™¨æœŸæœ›çš„å­—æ®µ
            'extrinsics': zeros_camera_params['extrinsic'][np.newaxis, ...],  # (1, 4, 4)
            'intrinsics': zeros_camera_params['intrinsic'][np.newaxis, ...],  # (1, 3, 3)
            'depths': zeros_depth[np.newaxis, ...],  # (1, H, W, 1) - è®­ç»ƒå™¨æœŸæœ›çš„å­—æ®µå
            'cam_points': zeros_point_cloud[np.newaxis, ...],  # (1, H*W, 3) - ç›¸æœºåæ ‡ç³»ç‚¹äº‘
            'point_masks': np.ones((1, height * width), dtype=np.bool_),  # (1, H*W) - ç‚¹äº‘æ©ç 
            'camera_params': {
                'extrinsic': zeros_camera_params['extrinsic'][np.newaxis, ...],  # (1, 4, 4)
                'intrinsic': zeros_camera_params['intrinsic'][np.newaxis, ...],  # (1, 3, 3)
            },
            'frame_index': seq_index,
            'total_frames': self.total_frames,
            'video_info': self.video_info,
        }
        
        if self.debug:
            logging.debug(f"ç”Ÿæˆæ•°æ® - å¸§ç´¢å¼•: {seq_index}, å›¾åƒå½¢çŠ¶: {data['images'].shape}")
        
        return data

    def __len__(self):
        """è¿”å›æ•°æ®é›†é•¿åº¦ï¼ˆæ€»å¸§æ•°ï¼‰"""
        return self.total_frames