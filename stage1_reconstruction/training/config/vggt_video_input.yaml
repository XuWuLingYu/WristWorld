defaults:
  - vggt_video_dataset.yaml

exp_name: vggt_video_input
img_size: 518  # Square image size divisible by patch_size=14 (1274=14Ã—91)
num_workers: 0  # Training workers
seed_value: 42
accum_steps: 3
patch_size: 14
limit_train_batches: 800
limit_val_batches: 400

# ğŸ”¥ è§†é¢‘è¾“å…¥æ¨¡å¼é…ç½®
video_input:
  enabled: True  # å¯ç”¨è§†é¢‘è¾“å…¥æ¨¡å¼
  ext1_video_path: "../fck/code/DiffSynth-Studio/data_processed/droid_autolab_success_move_1k/ext1/AUTOLab_success_AUTOLab+0d4edc83+2023-11-03-15h-42m-19s_ext1.mp4"  # ç¬¬ä¸€ä¸ªè¾“å…¥è§†é¢‘è·¯å¾„ï¼ˆext1ï¼‰
  ext2_video_path: "../fck/code/DiffSynth-Studio/data_processed/droid_autolab_success_move_1k/ext2/AUTOLab_success_AUTOLab+0d4edc83+2023-11-03-15h-42m-19s_ext2.mp4"  # ç¬¬äºŒä¸ªè¾“å…¥è§†é¢‘è·¯å¾„ï¼ˆext2ï¼‰
  wrist_video_path: "../fck/code/DiffSynth-Studio/data_processed/droid_autolab_success_move_1k/wrist/AUTOLab_success_AUTOLab+0d4edc83+2023-11-03-15h-42m-19s_wrist.mp4"  # GT wristè§†é¢‘è·¯å¾„
  # å½“å¯ç”¨è§†é¢‘è¾“å…¥æ¨¡å¼æ—¶ï¼Œtrain_set = val_set = è§†é¢‘çš„æ‰€æœ‰å¸§
  # å…¶ä»–GTæ•°æ®ï¼ˆç‚¹äº‘ã€æ·±åº¦ç­‰ï¼‰ç”¨zeroså¡«å……

# æ·»åŠ early validationé…ç½®
early_validation:
  enabled: True  # å¯ç”¨early validation
  step: 5        # åœ¨ç¬¬5æ­¥è¿›è¡Œearly validation
  limit_batches: 50  # é™åˆ¶early validationçš„batchæ•°é‡

logging:
  log_dir: logs
  log_visuals: True  # å¯ç”¨å¯è§†åŒ–
  log_freq: 1
  log_level_primary: INFO  # æ”¹ä¸ºINFOå‡å°‘debugä¿¡æ¯
  log_level_secondary: WARNING
  all_ranks: False
  tensorboard_writer:
    _target_: train_utils.tb_writer.TensorBoardLogger
    path: ${logging.log_dir}/tensorboard
  
  # å¢åŠ å¯è§†åŒ–é…ç½®
  log_visual_frequency:
    train: 100  # æ¯100æ­¥è®°å½•ä¸€æ¬¡è®­ç»ƒå¯è§†åŒ–
    val: 1     # æ¯ä¸ªepochè®°å½•éªŒè¯å¯è§†åŒ–
  
  visuals_keys_to_log:
    train:
      keys_to_log: ["depth", "world_points"]  # ä¿æŒå¯è§†åŒ–åŠŸèƒ½
      modality: "image"
    val:
      keys_to_log: ["depth", "world_points", "depth_conf"]  # ä¿æŒå¯è§†åŒ–åŠŸèƒ½
      modality: "image"
  
  visuals_per_batch_to_log: 4
  video_logging_fps: 10  # è§†é¢‘æ—¥å¿—å¸§ç‡
  scalar_keys_to_log:
    train:
      keys_to_log:
        - loss_objective
        - loss_camera  # ğŸ¯ åªä¿ç•™cameraç›¸å…³æŸå¤±
        - loss_T
        - loss_R
        - loss_FL
        # ğŸš« ç§»é™¤depthç›¸å…³æŸå¤±
        # - loss_conf_depth
        # - loss_reg_depth
        # - loss_grad_depth
        # ğŸš« ç§»é™¤pointç›¸å…³æŸå¤±
        # - loss_conf_point
        # - loss_reg_point
        # - loss_grad_point
        - loss_wrist  # ğŸ¯ ä¿ç•™wristç›¸å…³æŸå¤±
        - loss_wrist_T
        - loss_wrist_R
        - loss_wrist_FL
        - loss_projection  # ğŸ†• æ·»åŠ projection lossæ—¥å¿—
        - valid_track_points  # ğŸ†• æ·»åŠ æœ‰æ•ˆtrackç‚¹æ•°é‡
        - total_track_points  # ğŸ†• æ·»åŠ æ€»trackç‚¹æ•°é‡
        - depth_loss_count  # ğŸ†• æ·»åŠ æ·±åº¦æŸå¤±è®¡æ•°
        - uv_loss_sum  # ğŸ†• æ·»åŠ UVæŸå¤±æ€»å’Œ
        - depth_loss_sum  # ğŸ†• æ·»åŠ æ·±åº¦æŸå¤±æ€»å’Œ
    val:
      keys_to_log:
        - loss_objective
        - loss_camera  # ğŸ¯ åªä¿ç•™cameraç›¸å…³æŸå¤±
        - loss_T
        - loss_R
        - loss_FL
        # ğŸš« ç§»é™¤depthç›¸å…³æŸå¤±
        # - loss_conf_depth
        # - loss_reg_depth
        # - loss_grad_depth
        # ğŸš« ç§»é™¤pointç›¸å…³æŸå¤±
        # - loss_conf_point
        # - loss_reg_point
        # - loss_grad_point
        - loss_wrist  # ğŸ¯ ä¿ç•™wristç›¸å…³æŸå¤±
        - loss_wrist_T
        - loss_wrist_R
        - loss_wrist_FL
        - loss_projection  # ğŸ†• æ·»åŠ projection lossæ—¥å¿—
        - valid_track_points  # ğŸ†• æ·»åŠ æœ‰æ•ˆtrackç‚¹æ•°é‡
        - total_track_points  # ğŸ†• æ·»åŠ æ€»trackç‚¹æ•°é‡
        - depth_loss_count  # ğŸ†• æ·»åŠ æ·±åº¦æŸå¤±è®¡æ•°
        - uv_loss_sum  # ğŸ†• æ·»åŠ UVæŸå¤±æ€»å’Œ
        - depth_loss_sum  # ğŸ†• æ·»åŠ æ·±åº¦æŸå¤±æ€»å’Œ

checkpoint:
  save_dir: logs/${exp_name}/ckpts
  save_freq: 5
  resume_checkpoint_path: logs/vggt_droid_camera_only_finetune/ckpts/checkpoint.pt
  strict: False

loss:
  _target_: loss.MultitaskLoss
  camera: 
    weight: 5.0
    loss_type: "l1" # The paper uses smooth l1 loss, but we found l1 loss is more stable than smooth l1 and l2 loss.  
  depth:
    weight: 1.0
    gradient_loss_fn: "grad" 
    valid_range: 0.98
  point: null
  # If you want to enable point, use the following config
  # point: 
  #   weight: 1.0
  #   gradient_loss_fn: "normal" 
  #   valid_range: 0.98
  track: null   




optim:
  param_group_modifiers: False

  optimizer:
    _target_: torch.optim.AdamW
    lr: 1e-4
    weight_decay: 0.05

  frozen_module_names:
      - "*aggregator*"  # example, freeze the aggregator

  amp:
    enabled: True
    amp_dtype: bfloat16
  gradient_clip:
    _target_: train_utils.gradient_clip.GradientClipper
    configs:
      - module_name: ["aggregator"]
        max_norm: 1.0   # feel free to reduce this if you see instabilities
        norm_type: 2
      - module_name: ["depth"]
        max_norm: 1.0   # feel free to reduce this if you see instabilities
        norm_type: 2
      - module_name: ["camera"]
        max_norm: 1.0   # feel free to reduce this if you see instabilities
        norm_type: 2
      - module_name: ["point"]
        max_norm: 1.0   # ğŸ”¥ æ·»åŠ pointæ¨¡å—çš„æ¢¯åº¦è£å‰ª
        norm_type: 2
      - module_name: ["wrist"]
        max_norm: 1.0   # ğŸ”¥ æ·»åŠ wristæ¨¡å—çš„æ¢¯åº¦è£å‰ª
        norm_type: 2
  options:
    lr:
      - scheduler:
          _target_: fvcore.common.param_scheduler.CompositeParamScheduler
          schedulers:
            - _target_: fvcore.common.param_scheduler.LinearParamScheduler
              start_value: 1e-8
              end_value: 1e-4
            - _target_: fvcore.common.param_scheduler.CosineParamScheduler
              start_value: 1e-4
              end_value: 1e-8
          lengths: [0.05, 0.95]
          interval_scaling: ['rescaled', 'rescaled']
    weight_decay:
      - scheduler:
          _target_: fvcore.common.param_scheduler.ConstantParamScheduler
          value: 0.05




max_epochs: 100

model:
  _target_: vggt.models.vggt.VGGT
  enable_camera: True
  enable_depth: True
  enable_point: True  # ğŸ”¥ è§†é¢‘è¾“å…¥æ¨¡å¼éœ€è¦å¯ç”¨point_headä»¥ç”Ÿæˆworld_points
  enable_track: False


distributed:
  # check https://docs.pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html for options
  backend: nccl
  comms_dtype: None
  find_unused_parameters: False
  timeout_mins: 30
  gradient_as_bucket_view: True  # Less memory used
  bucket_cap_mb: 25
  broadcast_buffers: True

cuda:
    cudnn_deterministic: False
    cudnn_benchmark: False
    allow_tf32: True 